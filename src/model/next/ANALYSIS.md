# An√°lise Cr√≠tica: Next-POI Prediction Architecture

## Contexto
**Tarefa:** Prever a pr√≥xima categoria de POI dado hist√≥rico de visitas
**Input:** Sequ√™ncia de embeddings [B, seq_length=9, embed_dim]
**Output:** Logits [B, num_classes=7]

---

## ‚úÖ Transformer FAZ SENTIDO aqui! (mas pode melhorar)

### Por que √© diferente do Category Head?

| Aspecto | Category Head | Next-POI Head |
|---------|--------------|---------------|
| **Input** | 1 embedding √∫nico | Sequ√™ncia de 9 embeddings |
| **Tokens** | Artificiais (divis√£o de vetor) | Reais (cada visita) |
| **Sem√¢ntica** | Cada "token" sem significado | Cada token = 1 visita POI |
| **Ordem** | Irrelevante | **CRUCIAL** (temporal) |
| **Transformer?** | ‚ùå N√£o faz sentido | ‚úÖ Faz sentido |

---

## ‚úÖ Justificativa para usar Transformer

### 1. M√∫ltiplos embeddings independentes
```python
# Cada posi√ß√£o = uma visita diferente
x = [emb_restaurante, emb_cinema, emb_gym, emb_cafe, ...]
# Cada embedding tem significado pr√≥prio ‚úÖ
```

### 2. Sequencialidade temporal importa
```python
# Ordem altera completamente o padr√£o
[Gym ‚Üí Smoothie bar] ‚â† [Smoothie bar ‚Üí Gym]
                      ‚â† [Bar ‚Üí Cinema ‚Üí Gym]
```

### 3. Self-attention captura padr√µes de transi√ß√£o
```python
# Attention pode aprender:
# "Ap√≥s gym (80% das vezes) ‚Üí juice bar ou restaurante saud√°vel"
# "Ap√≥s cinema (60% das vezes) ‚Üí restaurante ou bar"
# "Sexta √† noite: restaurante ‚Üí bar (70%)"
```

### 4. Causal mask est√° correto
```python
# Linha 84-87 do c√≥digo atual:
causal_mask = torch.triu(torch.ones(...), diagonal=1)
# Previne "olhar para o futuro" ‚úÖ
# Essencial para next-POI prediction
```

---

## ‚ö†Ô∏è Problemas do Modelo Atual (NextHeadSingle)

### 1. Positional Encoding Sinusoidal
```python
# Implementa√ß√£o atual (linhas 18-25):
pe[:, 0::2] = torch.sin(pos * div_term)
pe[:, 1::2] = torch.cos(pos * div_term)
```

**Problema:**
- Sinusoidal PE foi projetado para **sequ√™ncias longas** (NLP: milhares de tokens)
- Permite generaliza√ß√£o para sequ√™ncias maiores que as vistas no treino
- **Seu caso: MAX_SEQ_LENGTH = 9** (muito curto!)

**Solu√ß√£o:**
```python
# Learned positional embeddings s√£o melhores para sequ√™ncias curtas
self.pos_embedding = nn.Parameter(torch.randn(1, 9, embed_dim))
```

**Por qu√™?**
- Pode aprender padr√µes espec√≠ficos da sua aplica√ß√£o
- Ex: "posi√ß√£o 0 (primeira visita) tem comportamento √∫nico"
- Mais par√¢metros, mas para seq=9 √© neglig√≠vel

---

### 2. Pooling Strategy: Ignora import√¢ncia temporal

```python
# C√≥digo atual (linhas 97-103):
attn_weights = torch.softmax(attn_logits, dim=1)
pooled = torch.sum(x * attn_weights, dim=1)
```

**Problema:**
- Trata todas as posi√ß√µes com import√¢ncia similar
- **Intui√ß√£o:** Visitas RECENTES importam mais que antigas!
- "Onde estive h√° 5 minutos" > "Onde estive ontem"

**Solu√ß√£o: Temporal Decay**
```python
# Peso exponencial: visitas recentes = mais importantes
decay = [e^-4, e^-3, e^-2, e^-1, e^0]
pooled = sum(x * decay * attention_weights)
```

---

### 3. Transformer pode ser overkill para 9 visitas

**Complexidade de Transformer:**
- Self-attention: O(seq_len¬≤)
- Para seq=9: 81 opera√ß√µes (ok)
- Para seq=100: 10,000 opera√ß√µes (a√≠ compensa)

**Alternativas mais eficientes:**
- **LSTM/GRU:** O(seq_len), projetados para sequ√™ncias
- **Temporal CNN:** Paralelo, captura padr√µes locais
- **Hybrid (GRU + Attention):** Melhor dos dois mundos

---

## üéØ Arquiteturas Recomendadas

### 1. **NextHeadGRU** ‚ö° Mais eficiente
**Quando usar:** Baseline para sequ√™ncias curtas
**Vantagens:**
- 25% menos par√¢metros que LSTM
- Implicitamente modela temporalidade (sem PE)
- Mais r√°pido que Transformer
- Perfeito para seq_length < 20

```python
NextHeadGRU(
    embed_dim=256,
    hidden_dim=256,
    num_classes=7,
    num_layers=2,
    dropout=0.3
)
```

**Complexidade:** ~150k params | Velocidade: ‚ö°‚ö°‚ö°‚ö°

---

### 2. **NextHeadLSTM** üß† Mem√≥ria de longo prazo
**Quando usar:** Se padr√µes t√™m depend√™ncias de longo alcance
**Vantagens:**
- C√©lula de mem√≥ria expl√≠cita (cell state)
- Melhor para sequ√™ncias com "contexto distante importante"
- Mais par√¢metros = mais capacidade

```python
NextHeadLSTM(
    embed_dim=256,
    hidden_dim=256,
    num_classes=7,
    num_layers=2,
    dropout=0.3,
    bidirectional=False  # True se ordem n√£o for estritamente causal
)
```

**Complexidade:** ~200k params | Velocidade: ‚ö°‚ö°‚ö°

---

### 3. **NextHeadHybrid** üèÜ RECOMENDADO
**Quando usar:** Melhor custo-benef√≠cio para seq=9
**Vantagens:**
- GRU processa sequ√™ncia eficientemente
- Self-attention foca em visitas importantes
- Interpret√°vel (pode visualizar attention weights)
- Combina strengths de RNN + Transformer

```python
NextHeadHybrid(
    embed_dim=256,
    hidden_dim=256,
    num_classes=7,
    num_heads=4,
    num_gru_layers=2,
    dropout=0.3
)
```

**Arquitetura:**
```
Input [B, 9, 256]
  ‚Üì
GRU (2 layers) ‚Üí Contexto sequencial
  ‚Üì
Self-Attention ‚Üí Foca em visitas relevantes
  ‚Üì
Residual Connection
  ‚Üì
Last timestep ‚Üí Classifier
```

**Complexidade:** ~250k params | Velocidade: ‚ö°‚ö°‚ö°

**Por que funciona:**
1. GRU captura depend√™ncias temporais
2. Attention seleciona "quais visitas importam agora"
3. Residual garante gradient flow

---

### 4. **NextHeadTemporalCNN** üöÄ Padr√µes locais
**Quando usar:** Se transi√ß√µes seguem padr√µes muito locais
**Vantagens:**
- Paralelo (mais r√°pido que RNN)
- Captura padr√µes tipo: "Gym ‚Üí Smoothie bar (sempre)"
- Receptive field cresce com camadas

```python
NextHeadTemporalCNN(
    embed_dim=256,
    hidden_channels=128,
    num_classes=7,
    num_layers=4,
    kernel_size=3,
    dropout=0.2
)
```

**Quando usar:**
- Padr√µes s√£o muito "pares consecutivos" (bigrams)
- Ex: "Restaurante ‚Üí Cinema", "Gym ‚Üí Cafe"
- Menos eficaz para depend√™ncias distantes

**Complexidade:** ~180k params | Velocidade: ‚ö°‚ö°‚ö°‚ö°‚ö°

---

### 5. **NextHeadTransformerOptimized** üîß Transformer melhorado
**Quando usar:** Se quiser manter Transformer, use esta vers√£o
**Melhorias sobre modelo atual:**

#### a) Learned Positional Embeddings
```python
# Antes (Sinusoidal):
pe[:, 0::2] = torch.sin(pos * div_term)

# Depois (Learned):
self.pos_embedding = nn.Parameter(torch.randn(1, seq_length, embed_dim))
```

#### b) Temporal Decay Pooling
```python
# Visitas recentes t√™m mais peso
decay = exp(-[4, 3, 2, 1, 0])  # [0.018, 0.05, 0.135, 0.37, 1.0]
pooled = sum(x * decay) / sum(decay)
```

#### c) Pre-norm (norm_first=True)
```python
# Mais est√°vel para redes profundas
encoder_layer = nn.TransformerEncoderLayer(..., norm_first=True)
```

```python
NextHeadTransformerOptimized(
    embed_dim=256,
    num_classes=7,
    num_heads=8,
    num_layers=2,
    seq_length=9,
    dropout=0.3,
    use_temporal_decay=True
)
```

**Complexidade:** ~220k params | Velocidade: ‚ö°‚ö°

---

## üìä Compara√ß√£o Geral

| Modelo | Params | Velocidade | Interpretabilidade | Adequa√ß√£o seq=9 |
|--------|--------|-----------|-------------------|-----------------|
| **GRU** | 150k | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **LSTM** | 200k | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Hybrid (GRU+Attn)** | 250k | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Temporal CNN** | 180k | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Transformer Opt** | 220k | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Transformer Atual** | 220k | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

---

## üß™ Plano de Experimenta√ß√£o Recomendado

### Fase 1: Baselines R√°pidos
```python
# 1. GRU (mais simples)
NextHeadGRU(embed_dim=256, hidden_dim=256, num_layers=2)

# 2. Transformer otimizado (compara√ß√£o justa)
NextHeadTransformerOptimized(embed_dim=256, num_layers=2, use_temporal_decay=True)
```

**Objetivo:** Estabelecer baseline s√≥lido rapidamente

---

### Fase 2: Explora√ß√£o
```python
# 3. Hybrid (recomendado)
NextHeadHybrid(embed_dim=256, hidden_dim=256, num_heads=4)

# 4. LSTM (se GRU saturar)
NextHeadLSTM(embed_dim=256, hidden_dim=256, num_layers=2)

# 5. Temporal CNN (se padr√µes forem locais)
NextHeadTemporalCNN(embed_dim=256, num_layers=4)
```

**Objetivo:** Encontrar melhor arquitetura

---

### Fase 3: Otimiza√ß√£o
Tunear o melhor da Fase 2:
- Hidden dim: [128, 256, 512]
- Num layers: [2, 3, 4]
- Dropout: [0.2, 0.3, 0.4]
- Learning rate schedule
- Temporal decay factor (se usando Transformer Opt)

---

### Fase 4: An√°lise
**Interpretabilidade (Hybrid ou Transformer):**
```python
# Visualizar attention weights
attn_weights = model.get_attention_weights(x)
# Quais visitas passadas s√£o mais relevantes?
# Restaurante ‚Üí ? (attn alto em 'delivery food')
```

**GRU/LSTM:**
```python
# Inspecionar hidden states
# t=8 (√∫ltima posi√ß√£o) deve conter todo contexto
```

---

## üí° Insights para Sequ√™ncias Curtas (seq=9)

### 1. RNNs s√£o subestimados
- **Mito:** "Transformers sempre > RNNs"
- **Realidade:** Para seq < 20, RNNs s√£o:
  - Mais eficientes
  - Mais f√°ceis de treinar
  - Performance similar ou melhor

### 2. Temporal decay √© crucial
```python
# Pr√≥xima visita depende MUITO da √∫ltima
# Correla√ß√£o temporal decai exponencialmente
visita[-1] (agora)    ‚Üí peso 1.0
visita[-2] (1h atr√°s) ‚Üí peso 0.37
visita[-5] (ontem)    ‚Üí peso 0.05
```

### 3. Padr√µes de transi√ß√£o s√£o chave
```python
# N√£o √© s√≥ "onde estou agora"
# √â "de onde vim ‚Üí para onde vou"
# Bigramas: (POI_t-1, POI_t) ‚Üí POI_t+1
```

**Hybrid captura isso bem:**
- GRU: contexto geral da trajet√≥ria
- Attention: foca na transi√ß√£o relevante

---

## üéì Quando usar cada arquitetura?

### Use **GRU** se:
- ‚úÖ Quer baseline r√°pido e eficiente
- ‚úÖ Sequ√™ncias s√£o curtas (< 20)
- ‚úÖ Padr√µes seguem ordem sequencial estrita
- ‚ùå N√£o precisa de interpretabilidade alta

### Use **LSTM** se:
- ‚úÖ GRU saturou (mais capacidade)
- ‚úÖ Contexto distante importa (mem√≥ria de longo prazo)
- ‚ùå Velocidade n√£o √© cr√≠tica

### Use **Hybrid (GRU + Attention)** se: üëë
- ‚úÖ Quer melhor performance geral
- ‚úÖ Interpretabilidade √© importante (attention weights)
- ‚úÖ Tem dados suficientes (~10k+ samples)
- ‚úÖ Pode pagar custo computacional moderado

### Use **Temporal CNN** se:
- ‚úÖ Padr√µes s√£o majoritariamente locais (bigrams, trigrams)
- ‚úÖ Precisa de velocidade m√°xima
- ‚úÖ Pode treinar em paralelo (GPUs)
- ‚ùå N√£o precisa de depend√™ncias muito distantes

### Use **Transformer** se:
- ‚úÖ Quer modelar rela√ß√µes complexas entre todas as visitas
- ‚úÖ Sequ√™ncias podem ser mais longas no futuro
- ‚úÖ Tem GPU forte
- ‚ö†Ô∏è **Use vers√£o otimizada** (learned PE + temporal decay)

---

## üìö Refer√™ncias

### Papers relevantes:
- **DeepMove (IJCAI 2018):** RNN + Attention para next-location
- **STAN (KDD 2020):** Spatial-Temporal Attention para POI
- **LSTM vs Transformer (2021):** Compara√ß√£o para s√©ries curtas

### Conclus√µes da literatura:
> "Para s√©ries temporais curtas (< 50 timesteps), LSTMs e Transformers t√™m
> performance similar, mas LSTMs s√£o mais eficientes" - Zerveas et al., 2021

> "Hybrid architectures (RNN + Attention) superam ambos para next-item prediction
> com sequ√™ncias m√©dias" - Kang & McAuley, ICDM 2018

---

## üéØ Recomenda√ß√£o Final

**Para next-POI com seq_length=9:**

### Top 3 escolhas:

1. **NextHeadHybrid** ü•á
   - Melhor custo-benef√≠cio
   - Performance robusta
   - Interpret√°vel

2. **NextHeadGRU** ü•à
   - Baseline s√≥lido
   - Mais r√°pido
   - √ìtimo ponto de partida

3. **NextHeadTransformerOptimized** ü•â
   - Se j√° usa Transformer
   - Melhorias significativas sobre vers√£o atual
   - Mant√©m compatibilidade

---

**Regra de ouro para Next-POI:**
> "Sequ√™ncias curtas favorecem RNNs e hybrids.
> Use Transformer apenas se otimizado para seu caso."

**Pr√≥ximos passos sugeridos:**
1. Implementar GRU como baseline r√°pido
2. Testar Hybrid para compara√ß√£o
3. Analisar attention weights (interpretabilidade)
4. Otimizar hiperpar√¢metros do melhor modelo
---

## üêõ Bugfixes e Corre√ß√µes

### TemporalCNN: BatchNorm vs LayerNorm

**Problema identificado (2025-11-07):**
```python
# ERRADO - causava RuntimeError
nn.Conv1d(in_channels, out_channels, kernel_size)
nn.LayerNorm(out_channels)  # ‚ùå
# Shape ap√≥s Conv1d: [batch, channels, length] = [1024, 128, 11]
# LayerNorm esperava: [*, 128] (128 na √∫ltima dim)
# Mas 128 est√° no MEIO (channels)!
```

**Erro:**
```
RuntimeError: Given normalized_shape=[128], expected input with shape [*, 128], 
but got input of size[1024, 128, 11]
```

**Solu√ß√£o:**
```python
# CORRETO
nn.Conv1d(in_channels, out_channels, kernel_size)
nn.BatchNorm1d(out_channels)  # ‚úÖ
# BatchNorm1d normaliza sobre channels (dim 1)
# Funciona com shape [batch, channels, length]
```

**Li√ß√£o aprendida:**
- Para **Conv1d**: Use `BatchNorm1d` (normaliza channels)
- Para **Linear/Transformer**: Use `LayerNorm` (normaliza √∫ltima dim)
- Conv1d shape: `[B, C, L]` ‚Üí BatchNorm1d(C)
- Transformer shape: `[B, L, D]` ‚Üí LayerNorm(D)
