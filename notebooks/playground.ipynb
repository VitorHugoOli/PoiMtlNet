{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T00:46:39.679566Z",
     "start_time": "2025-05-05T00:46:37.493048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from configs.model import InputsConfig\n",
    "from modeling.category_single import POIDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from configs.globals import DEVICE\n",
    "\n",
    "\n",
    "# --- your pre‐processing steps ---\n",
    "df = pd.read_csv('/Users/vitor/Desktop/mestrado/ingred/data/output/florida_test/pre-processing/next-input.csv')\n",
    "df['y'] = df['next_category'].astype('category').cat.codes\n",
    "df.drop(columns=['userid', 'next_category'], inplace=True)\n",
    "feature_cols = df.columns[0:InputsConfig.EMBEDDING_DIM * 9]\n",
    "df['x'] = df[feature_cols].values.tolist()\n",
    "\n",
    "\n",
    "# prepare raw lists\n",
    "X = df['x'].tolist()\n",
    "y = df['y'].tolist()\n",
    "\n",
    "# --- create 5‐fold splits ---\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "folds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), 1):\n",
    "    # slice out this fold’s train/val\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    y_train = [y[i] for i in train_idx]\n",
    "    X_val = [X[i] for i in val_idx]\n",
    "    y_val = [y[i] for i in val_idx]\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], 9, InputsConfig.EMBEDDING_DIM)\n",
    "    X_val = X_val.reshape(X_val.shape[0], 9, InputsConfig.EMBEDDING_DIM)\n",
    "\n",
    "    # build Datasets\n",
    "    train_ds = POIDataset(X_train, y_train)\n",
    "    val_ds = POIDataset(X_val, y_val)\n",
    "\n",
    "    # and DataLoaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=512, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "    folds.append((train_loader, val_loader))\n",
    "\n",
    "    print(f\"Fold {fold}: train={len(train_ds)}  val={len(val_ds)}\")"
   ],
   "id": "56bd44eebc53fffc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: train=13093  val=3274\n",
      "Fold 2: train=13093  val=3274\n",
      "Fold 3: train=13094  val=3273\n",
      "Fold 4: train=13094  val=3273\n",
      "Fold 5: train=13094  val=3273\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T00:49:53.409083Z",
     "start_time": "2025-05-05T00:49:53.407151Z"
    }
   },
   "cell_type": "code",
   "source": "PAD_VALUE = -999",
   "id": "95f915d74cb2ec2a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T00:55:10.026001Z",
     "start_time": "2025-05-05T00:55:10.018815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class WindowClassifierWithTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=100, hidden_dim=64, num_classes=7,\n",
    "                 num_layers=2, nhead=8, dropout=0.1, num_windows=9):\n",
    "        super(WindowClassifierWithTransformer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, num_windows, hidden_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        self.reduction_layer = nn.Linear(num_windows * num_classes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        flattened = logits.reshape(logits.size(0), logits.size(1) * logits.size(2))\n",
    "\n",
    "        final_logits = self.reduction_layer(flattened)  # [batch_size, num_classes]\n",
    "\n",
    "        final_probs = F.softmax(final_logits, dim=-1)  # [batch_size, num_classes]\n",
    "\n",
    "        return final_probs"
   ],
   "id": "e07699bdb27ade02",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T18:05:07.483569Z",
     "start_time": "2025-05-04T18:05:07.477163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RepeatedTransformerClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=107,\n",
    "                 d_model=128,\n",
    "                 nhead=8,\n",
    "                 num_layers=6,  # <-- number of times to repeat\n",
    "                 num_classes=7,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 1) lift input to model dimension\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        # 2) positional encoding\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=9)\n",
    "\n",
    "        # 3) build your own stack of layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=d_model * 4,\n",
    "                dropout=dropout,\n",
    "                norm_first=True      # ← switch to pre-norm\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # 4) final classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len=9, input_dim=107)\n",
    "        returns: (batch, num_classes=7)\n",
    "        \"\"\"\n",
    "        # project → (batch, 9, d_model)\n",
    "        x = self.input_proj(x)\n",
    "        # to (seq, batch, d_model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # add positional info\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        # now *repeat* self-attention+MLP num_layers times\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # → still (9, batch, d_model)\n",
    "\n",
    "        # take last step’s vector: shape (batch, d_model)\n",
    "        last_vec = x.mean(dim=0)\n",
    "        # classify → (batch, 7)\n",
    "        logits = self.classifier(last_vec)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "                             * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch, d_model)\n",
    "        return x + self.pe[:x.size(0)]"
   ],
   "id": "e623a09bbffd1bd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T00:53:28.709268Z",
     "start_time": "2025-05-05T00:53:28.695494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_length=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, embed_dim)\n",
    "        pos = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, embed_dim, 2).float() *\n",
    "                        (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        pe = pe.unsqueeze(0)           # (1, max_seq_length, embed_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class NextHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_classes: int,\n",
    "                 num_heads: int,\n",
    "                 seq_length: int,\n",
    "                 num_layers: int,\n",
    "                 dropout: float = 0.35):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.pad_value = PAD_VALUE\n",
    "\n",
    "        # positional encoding for up to seq_length\n",
    "        self.pe = PositionalEncoding(embed_dim, max_seq_length=seq_length)\n",
    "\n",
    "        # a single TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # project each position to a single score (we’ll do softmax in forward)\n",
    "        self.score_proj = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        # final classification\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_length, embed_dim),\n",
    "           padded positions are exactly all elements == self.pad_value\n",
    "        \"\"\"\n",
    "        # 1) build padding mask\n",
    "        #    mask[b, i] = True  =>  position i in batch b is padding\n",
    "        pad_mask = (x == self.pad_value).all(dim=-1)  # (batch, seq_length)\n",
    "\n",
    "        # 2) add positional encodings\n",
    "        x = self.pe(x)  # (batch, seq_length, embed_dim)\n",
    "\n",
    "        # 3) build a boolean causal mask of shape (seq_length, seq_length)\n",
    "        #    True indicates positions that should NOT attend (i.e. upper triangular)\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones((self.seq_length, self.seq_length),\n",
    "                       dtype=torch.bool,\n",
    "                       device=x.device),\n",
    "            diagonal=1\n",
    "        )\n",
    "        x = self.transformer(\n",
    "            x,\n",
    "            mask=causal_mask,                # now dtype=bool\n",
    "            src_key_padding_mask=pad_mask    # also dtype=bool\n",
    "        )\n",
    "\n",
    "        # 4) compute a weight for each position\n",
    "        #    then mask out padding before softmax\n",
    "        scores = self.score_proj(x).squeeze(-1)       # (batch, seq_length)\n",
    "        scores = scores.masked_fill(pad_mask, float('-inf'))\n",
    "        attn_weights = torch.softmax(scores, dim=1)   # (batch, seq_length)\n",
    "\n",
    "        # 5) weighted sum over sequence\n",
    "        #    (batch, embed_dim)\n",
    "        pooled = torch.einsum('bs,bse->be', attn_weights, x)\n",
    "\n",
    "        # 6) final classification\n",
    "        out = self.classifier(pooled)                 # (batch, num_classes)\n",
    "        return out"
   ],
   "id": "27939d5e8bbb9f5b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T00:46:44.037040Z",
     "start_time": "2025-05-05T00:46:44.022833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_length=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create constant positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, embed_dim)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if embed_dim % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, seq_length, embed_dim)\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class NextHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=64,\n",
    "        num_classes=7,\n",
    "        num_heads=4,\n",
    "        max_seq_length=9,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Make transformer hidden size divisible by num_heads\n",
    "        self.transformer_dim = num_heads * math.ceil(embed_dim / num_heads)\n",
    "\n",
    "        # Project input embeddings to transformer dimension\n",
    "        self.input_proj = nn.Linear(embed_dim, self.transformer_dim)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_enc = PositionalEncoding(self.transformer_dim, max_seq_length, dropout)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.transformer_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=self.transformer_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # Attention pooling\n",
    "        self.attn_fc = nn.Linear(self.transformer_dim, 1)\n",
    "\n",
    "        # Final classifier\n",
    "        self.output_fc = nn.Linear(self.transformer_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length, embed_dim)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # Create padding mask: True for positions that are all PAD_VALUE\n",
    "        pad_mask = (x == PAD_VALUE).all(dim=-1)  # (batch_size, seq_length)\n",
    "\n",
    "        # Project and add positional encoding\n",
    "        x_proj = self.input_proj(x)\n",
    "        x_proj = self.pos_enc(x_proj)\n",
    "\n",
    "        # Causal mask to prevent attending to future positions\n",
    "        causal_mask = torch.triu(\n",
    "            torch.full((seq_length, seq_length), float('-inf'), device=x.device),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "        # Apply transformer encoder\n",
    "        enc_out = self.transformer(\n",
    "            x_proj,\n",
    "            mask=causal_mask,\n",
    "            src_key_padding_mask=pad_mask,\n",
    "        )  # (batch_size, seq_length, transformer_dim)\n",
    "\n",
    "        # Compute attention logits and mask padding\n",
    "        attn_logits = self.attn_fc(enc_out).squeeze(-1)  # (batch_size, seq_length)\n",
    "        attn_logits = attn_logits.masked_fill(pad_mask, float('-inf'))\n",
    "\n",
    "        # Normalize to get weights\n",
    "        attn_weights = torch.softmax(attn_logits, dim=-1)  # (batch_size, seq_length)\n",
    "\n",
    "        # Weighted sum of sequence outputs\n",
    "        summary = torch.bmm(attn_weights.unsqueeze(1), enc_out).squeeze(1)  # (batch_size, transformer_dim)\n",
    "\n",
    "        # Final classification\n",
    "        out = self.output_fc(summary)\n",
    "        return out"
   ],
   "id": "4eba585aabb8fc1b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T05:43:33.266820Z",
     "start_time": "2025-04-26T05:43:33.256629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class POICategoryPredictor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 107,   # dimensionality of each POI embedding\n",
    "        hidden_size: int = 128,  # number of features in the LSTM hidden state\n",
    "        num_layers: int = 2,     # number of stacked LSTM layers\n",
    "        num_classes: int = 7,    # number of categories to predict\n",
    "        dropout: float = 0.3     # dropout between LSTM layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # batch_first=True so input / output tensors are (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        # if you want to add more capacity you can insert extra Linear+ReLU here\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, seq_len=9, input_size=107)\n",
    "        returns: logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # LSTM returns output for all time steps and the final hidden state\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        # h_n is (num_layers, batch, hidden_size); we want the last layer\n",
    "        last_hidden = h_n[-1]             # shape: (batch_size, hidden_size)\n",
    "        logits = self.classifier(last_hidden)\n",
    "        return logits"
   ],
   "id": "22f6b5c4b2572ad",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T18:07:55.517812Z",
     "start_time": "2025-05-04T18:07:55.512389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class POISequencePredictor(nn.Module):\n",
    "    def __init__(self, input_dim=107, hidden_dim=128, num_heads=4, num_layers=2, num_classes=7, dropout=0.1):\n",
    "        super(POISequencePredictor, self).__init__()\n",
    "\n",
    "        # Embedding dimension is already provided (107)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Positional encoding for the sequence\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, 9, hidden_dim))\n",
    "\n",
    "        # Project input embeddings to the hidden dimension\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Final prediction layer\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length=9, embedding_dim=107)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Project input to hidden dimension\n",
    "        x = self.input_projection(x)  # shape: (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        # Pass through transformer\n",
    "        x = self.transformer_encoder(x)  # shape: (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "        # Get the last position's output\n",
    "        last_hidden = x[:, -1, :]  # shape: (batch_size, hidden_dim)\n",
    "\n",
    "        # Predict next category\n",
    "        logits = self.classifier(last_hidden)  # shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits"
   ],
   "id": "11000af41bf3e66d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T00:56:05.320255Z",
     "start_time": "2025-05-05T00:55:16.836954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for fold_idx, (train_loader, val_loader) in enumerate(folds):\n",
    "    fold_metrics = defaultdict(list)\n",
    "\n",
    "    # model = POISequencePredictor(\n",
    "    #     input_dim=InputsConfig.EMBEDDING_DIM,\n",
    "    #     hidden_dim=128,\n",
    "    #     num_heads=4,\n",
    "    #     num_layers=2,\n",
    "    #     num_classes=7,\n",
    "    #     dropout=0.4\n",
    "    # )\n",
    "    # model = NextHead(\n",
    "    #     embed_dim=InputsConfig.EMBEDDING_DIM,\n",
    "    #     num_classes=7,\n",
    "    #     num_heads=4,\n",
    "    #     seq_length=9,\n",
    "    #     num_layers=2,\n",
    "    #     dropout=0.35\n",
    "    # )\n",
    "    model = WindowClassifierWithTransformer(\n",
    "        input_dim=InputsConfig.EMBEDDING_DIM,\n",
    "        hidden_dim=128,\n",
    "        num_classes=7,\n",
    "        num_layers=2,\n",
    "        nhead=4,\n",
    "        dropout=0.1,\n",
    "        num_windows=9\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Enhanced optimizer setup\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=5e-5,\n",
    "        eps=1e-3,  # Helps prevent division by zero\n",
    "        weight_decay=0.01  # Slightly stronger regularization\n",
    "    )\n",
    "\n",
    "    # Transformer-specific learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=2e-4,  # Peak learning rate\n",
    "        total_steps=200 * len(train_loader),  # Total number of training steps\n",
    "        pct_start=0.1,  # Percentage of steps for warmup\n",
    "        div_factor=25,  # Initial lr = max_lr/div_factor\n",
    "        final_div_factor=10000,  # Final lr = max_lr/(div_factor*final_div_factor)\n",
    "        anneal_strategy='cos'  # Cosine annealing\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    epoch_progress = tqdm(range(200), desc=f\"Fold {fold_idx}\")\n",
    "\n",
    "    for epoch in epoch_progress:\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            x_next = X_batch.to(DEVICE, non_blocking=True)\n",
    "            y_next = y_batch.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out_a = model(x_next)\n",
    "            loss = criterion(out_a, y_next)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            _, predicted = torch.max(out_a, 1)\n",
    "            correct = (predicted == y_next).sum().item()\n",
    "            total = y_next.size(0)\n",
    "\n",
    "            train_loss += loss.item() * total\n",
    "            train_acc += correct\n",
    "            train_total += total\n",
    "\n",
    "        epoch_train_loss = train_loss / train_total\n",
    "        epoch_train_acc = train_acc / train_total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                x_next = X_batch.to(DEVICE, non_blocking=True)\n",
    "                y_next = y_batch.to(DEVICE, non_blocking=True)\n",
    "\n",
    "                out_a = model(x_next)\n",
    "                loss = criterion(out_a, y_next)\n",
    "\n",
    "                _, predicted = torch.max(out_a, 1)\n",
    "                correct = (predicted == y_next).sum().item()\n",
    "                total = y_next.size(0)\n",
    "\n",
    "                val_loss += loss.item() * total\n",
    "                val_acc += correct\n",
    "                val_total += total\n",
    "\n",
    "        epoch_val_loss = val_loss / val_total\n",
    "        epoch_val_acc = val_acc / val_total\n",
    "\n",
    "        fold_metrics['train_loss'].append(epoch_train_loss)\n",
    "        fold_metrics['train_acc'].append(epoch_train_acc)\n",
    "        fold_metrics['val_loss'].append(epoch_val_loss)\n",
    "        fold_metrics['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "        epoch_progress.set_postfix({\n",
    "            'tr_loss': f\"{epoch_train_loss:.4f}\",\n",
    "            'tr_acc': f\"{epoch_train_acc:.4f}\",\n",
    "            'vl_loss': f\"{epoch_val_loss:.4f}\",\n",
    "            'vl_acc': f\"{epoch_val_acc:.4f}\"\n",
    "        })\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted = []\n",
    "        ground_truth = []\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            x_next = X_batch.to(DEVICE, non_blocking=True)\n",
    "            y_next = y_batch.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            out_a = model(x_next)\n",
    "\n",
    "            _, pred = torch.max(out_a, 1)\n",
    "            predicted.append(pred.cpu().numpy())\n",
    "            ground_truth.append(y_next.cpu().numpy())\n",
    "\n",
    "        report = classification_report(\n",
    "            np.concatenate(ground_truth),\n",
    "            np.concatenate(predicted),\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(json.dumps(report, indent=4))\n",
    "\n",
    "    print(f\"Fold {fold_idx} - Best Val Acc: {best_val_acc:.4f}\")"
   ],
   "id": "f0c5d9cf58f7949d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitor/Desktop/mestrado/ingred/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fold 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97948f2f0fe64b588f1f8475ae2e481b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 97\u001B[0m\n\u001B[1;32m     94\u001B[0m val_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m X_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m val_loader:\n\u001B[1;32m     98\u001B[0m         x_next \u001B[38;5;241m=\u001B[39m X_batch\u001B[38;5;241m.\u001B[39mto(DEVICE, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     99\u001B[0m         y_next \u001B[38;5;241m=\u001B[39m y_batch\u001B[38;5;241m.\u001B[39mto(DEVICE, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Desktop/mestrado/ingred/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 708\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    712\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    714\u001B[0m ):\n",
      "File \u001B[0;32m~/Desktop/mestrado/ingred/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:764\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    763\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 764\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    766\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cfb2406857dc8338",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output.shape",
   "id": "bbc0c88b8201e7a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Cross-Stitch Unit\n",
    "class CrossStitchUnit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossStitchUnit, self).__init__()\n",
    "        # Learnable alpha parameters (initialized to identity)\n",
    "        self.alpha = nn.Parameter(torch.tensor([[0.9, 0.1], [0.1, 0.9]], requires_grad=True))\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        # a and b are features from each task branch\n",
    "        a_out = self.alpha[0, 0] * a + self.alpha[0, 1] * b\n",
    "        b_out = self.alpha[1, 0] * a + self.alpha[1, 1] * b\n",
    "        return a_out, b_out\n",
    "\n",
    "\n",
    "# A simple convolutional block\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pool(F.relu(self.conv(x)))\n",
    "\n",
    "\n",
    "# Main Multi-Task Network with Cross-Stitch\n",
    "class CrossStitchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossStitchNet, self).__init__()\n",
    "        # Task-specific initial blocks\n",
    "        self.taskA_conv1 = ConvBlock()\n",
    "        self.taskB_conv1 = ConvBlock()\n",
    "\n",
    "        # Cross-stitch unit after first conv layer\n",
    "        self.cross_stitch = CrossStitchUnit()\n",
    "\n",
    "        # Shared second conv block\n",
    "        self.taskA_conv2 = ConvBlock()\n",
    "        self.taskB_conv2 = ConvBlock()\n",
    "\n",
    "        # Task-specific heads\n",
    "        self.taskA_fc = nn.Linear(16 * 7 * 7, 10)  # For classification\n",
    "        self.taskB_fc = nn.Linear(16 * 7 * 7, 1)  # For regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.taskA_conv1(x)\n",
    "        b = self.taskB_conv1(x)\n",
    "\n",
    "        # Cross-stitch blending\n",
    "        a, b = self.cross_stitch(a, b)\n",
    "\n",
    "        # Continue task-specific paths\n",
    "        a = self.taskA_conv2(a)\n",
    "        b = self.taskB_conv2(b)\n",
    "\n",
    "        # Flatten\n",
    "        a = a.view(a.size(0), -1)\n",
    "        b = b.view(b.size(0), -1)\n",
    "\n",
    "        # Final heads\n",
    "        outA = self.taskA_fc(a)\n",
    "        outB = self.taskB_fc(b)\n",
    "        return outA, outB"
   ],
   "id": "232717a0eadc78e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.randn(32, 1, 28, 28)  # Example input\n",
    "\n",
    "model = CrossStitchNet()\n",
    "output = model(x)"
   ],
   "id": "3095e80c46e1c29d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output.shape",
   "id": "9c65aca2fc68d76d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "79376cba1b317c04",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
