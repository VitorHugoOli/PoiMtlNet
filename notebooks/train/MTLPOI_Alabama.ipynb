{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "O modelo Multi-Task Learning é composto por uma ou mais tarefas que são treinadas de forma conjunta. O modelo aqui desenvolvido é o MTL-POI e são treinadas duas tarefas de forma conjunta, ou seja, dois modelos Single-Task:\n",
        "\n",
        "*   NPC -> NextPoiCat\n",
        "*   PCat -> PoiCategorization"
      ],
      "metadata": {
        "id": "DvNsIwtFO2oz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9dShepm23If",
        "outputId": "c1f53ed6-16ed-4225-852c-78cf8959a297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JkJj-fPIG27"
      },
      "source": [
        "### **Bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade calflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX6cCU9Tfq7l",
        "outputId": "8670c4b8-9bfd-468b-aaa7-0596d671a997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: calflops in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from calflops) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from calflops) (0.28.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from calflops) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.22.0->calflops) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.22.0->calflops) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.22.0->calflops) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.22.0->calflops) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.22.0->calflops) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->calflops) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->calflops) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->calflops) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->calflops) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->calflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->calflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.1.0->calflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.1.0->calflops) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pnPFkaeoh-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a1bbe656-2e13-48ca-a53c-c03b457fd343"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8f81273bbbd5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcalflops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalculate_flops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/calflops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m '''\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mflops_counter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalculate_flops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mflops_counter_hf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalculate_flops_hf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/calflops/flops_counter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import geopandas\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "from typing import Iterator\n",
        "\n",
        "from calflops import calculate_flops\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import ast\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "from itertools import cycle\n",
        "from tabulate import tabulate\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configurações**"
      ],
      "metadata": {
        "id": "mDlyO8AOeSOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0CThKCj4Kee"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Treinamento usando GPU.\")\n",
        "else:\n",
        "    print(\"Treinamento usando CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 18\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "xmf1Y_7adwRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg4qrVHuNSBA"
      },
      "source": [
        "#### **Definição dos *paths* das entradas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ela_A1NWAN"
      },
      "outputs": [],
      "source": [
        "# alabama\n",
        "path_nextpoi_input_alabama = '/content/drive/MyDrive/Graduacao/POC/Dados/pre-processing/alabama/nextpoi-input.csv'\n",
        "path_categorypoi_input_alabama = '/content/drive/MyDrive/Graduacao/POC/Dados/pre-processing/alabama/categorypoi-input.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNpjU-2M4SZq"
      },
      "source": [
        "#### **Inicialização do MlFlow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li9lectC4VjK"
      },
      "outputs": [],
      "source": [
        "!pip install mlflow\n",
        "import mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1-3fiqK4X_j"
      },
      "outputs": [],
      "source": [
        "DAGSHUB_USER = \"ingredalmeida1\"\n",
        "DAGSHUB_REPO = \"poc\"\n",
        "DAGSHUB_TOKEN = \"fcd26b772e42ea1a5e95651a69fb0adb7bd00e2e\"\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = DAGSHUB_USER\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = DAGSHUB_TOKEN\n",
        "DAGSHUB_URL = f\"https://dagshub.com/{DAGSHUB_USER}/{DAGSHUB_REPO}.mlflow\"\n",
        "mlflow.set_tracking_uri(DAGSHUB_URL)\n",
        "mlflow.set_experiment(\"alabama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY3646SHTtkl"
      },
      "source": [
        "### **Funções utilitárias**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkaE_93wjEly"
      },
      "source": [
        "#### **Imprimir métricas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NXny8jvzJ_t"
      },
      "outputs": [],
      "source": [
        "id_to_name = {\n",
        "    '0': 'Community',\n",
        "    '1': 'Entertainment',\n",
        "    '2': 'Food',\n",
        "    '3': 'Nightlife',\n",
        "    '4': 'Outdoors',\n",
        "    '5': 'Shopping',\n",
        "    '6': 'Travel'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLTMDsh4zQ2B"
      },
      "outputs": [],
      "source": [
        "def print_classification_report_train(y_true_category, y_pred_category, y_true_next, y_pred_next, id_to_name):\n",
        "    category_report = classification_report(y_true_category, y_pred_category, zero_division=1, output_dict=True)\n",
        "    next_report = classification_report(y_true_next, y_pred_next, zero_division=1, output_dict=True)\n",
        "\n",
        "    category_df = pd.DataFrame(category_report).transpose()\n",
        "    next_df = pd.DataFrame(next_report).transpose()\n",
        "\n",
        "    category_df = category_df.drop(columns='support')\n",
        "    category_df = category_df.drop(index=['weighted avg', 'accuracy'])\n",
        "\n",
        "    next_df = next_df.drop(columns='support')\n",
        "    next_df = next_df.drop(index=['weighted avg', 'accuracy'])\n",
        "\n",
        "    category_df.rename(index=id_to_name, inplace=True)\n",
        "    next_df.rename(index=id_to_name, inplace=True)\n",
        "\n",
        "    category_df = category_df.transpose()\n",
        "    next_df = next_df.transpose()\n",
        "\n",
        "    category_df = category_df.map(lambda x: f\"{x * 100:.1f}\")\n",
        "    next_df = next_df.map(lambda x: f\"{x * 100:.1f}\")\n",
        "\n",
        "    print(\"\\ncategory train metrics:\")\n",
        "    print(category_df.to_string())\n",
        "\n",
        "    print(\"\\n next train metrics:\")\n",
        "    print(next_df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSSAwExE6uQd"
      },
      "outputs": [],
      "source": [
        "def print_classification_report_test(y_true, y_pred, id_to_name, task_name):\n",
        "    report = classification_report(y_true, y_pred, zero_division=1, output_dict=True)\n",
        "\n",
        "    metrics_df = pd.DataFrame(report).transpose()\n",
        "    metrics_df = metrics_df.drop(columns='support')\n",
        "    metrics_df = metrics_df.drop(index=['weighted avg', 'accuracy'])\n",
        "\n",
        "    metrics_df.rename(index=id_to_name, inplace=True)\n",
        "    metrics_df = metrics_df.transpose()\n",
        "\n",
        "    metrics_df = metrics_df.map(lambda x: f\"{x * 100:.1f}\")\n",
        "\n",
        "    print(f'{task_name} test metrics:')\n",
        "    print(metrics_df.to_string() + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2ICHVcM8-oB"
      },
      "outputs": [],
      "source": [
        "def print_classification_report_val(y_true, y_pred, id_to_name, task_name):\n",
        "    report = classification_report(y_true, y_pred, zero_division=1, output_dict=True)\n",
        "\n",
        "    metrics_df = pd.DataFrame(report).transpose()\n",
        "    metrics_df = metrics_df.drop(columns='support')\n",
        "    metrics_df = metrics_df.drop(index=['weighted avg', 'accuracy'])\n",
        "\n",
        "    metrics_df.rename(index=id_to_name, inplace=True)\n",
        "    metrics_df = metrics_df.transpose()\n",
        "\n",
        "    metrics_df = metrics_df.map(lambda x: f\"{x * 100:.1f}\")\n",
        "\n",
        "    print(f'validação {task_name} metrics:')\n",
        "    print(metrics_df.to_string() + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1FiLVSsj_Y7"
      },
      "source": [
        "#### **Imprimir perdas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG_iTubLkByI"
      },
      "outputs": [],
      "source": [
        "def print_train_losses(epoch, num_epochs, mtl_loss, next_loss, category_losss):\n",
        "  print(f'\\nEPOCH {epoch + 1}/{num_epochs}:')\n",
        "  print(f'mtl loss: {mtl_loss:.1f}')\n",
        "  print(f'next loss: {next_loss:.1f}')\n",
        "  print(f'category loss: {category_losss:.1f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_val_losses(next_loss, category_losss):\n",
        "  print(f'\\nVALIDATION LOSSES:')\n",
        "  print(f'next val loss: {next_loss:.1f}')\n",
        "  print(f'category val loss: {category_losss:.1f}\\n')"
      ],
      "metadata": {
        "id": "PbV_yGTYqMiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfx1F_2aq9Rc"
      },
      "source": [
        "#### **Cálculo das métricas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxFFg2-pLCOh"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics_by_fold(fold_results, task_name):\n",
        "    metrics = {\n",
        "        'fold': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1-score': []\n",
        "    }\n",
        "\n",
        "    for fold, results in fold_results.items():\n",
        "        y_true = results[f'test_{task_name}_true']\n",
        "        y_pred = results[f'test_{task_name}_pred']\n",
        "\n",
        "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n",
        "\n",
        "        metrics['fold'].append(fold)\n",
        "        metrics['precision'].append(report['macro avg']['precision'])\n",
        "        metrics['recall'].append(report['macro avg']['recall'])\n",
        "        metrics['f1-score'].append(report['macro avg']['f1-score'])\n",
        "\n",
        "    return pd.DataFrame(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77IpbaZOSo-F"
      },
      "source": [
        "### **Input do modelo MTL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh3d55fg5kgX"
      },
      "outputs": [],
      "source": [
        "def x_y_to_tensor(x, y, task_name):\n",
        "  x = torch.tensor(x.values, dtype=torch.float)\n",
        "  y = torch.tensor(y.values, dtype=torch.long)\n",
        "\n",
        "  if task_name == 'next':\n",
        "     x = x.view(-1, 9, 100)\n",
        "  else:\n",
        "     x = x.view(-1, 1, 100)\n",
        "\n",
        "\n",
        "  x.to(device)\n",
        "  y.to(device)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "def map_categories(y):\n",
        "  id_to_name = {\n",
        "    '0': 'Community',\n",
        "    '1': 'Entertainment',\n",
        "    '2': 'Food',\n",
        "    '3': 'Nightlife',\n",
        "    '4': 'Outdoors',\n",
        "    '5': 'Shopping',\n",
        "    '6': 'Travel',\n",
        "    '7': 'None'\n",
        "  }\n",
        "\n",
        "  name_to_id = {name: int(id) for id, name in id_to_name.items()}\n",
        "  try:\n",
        "    y_encoded = np.array([[name_to_id[yi] for yi in row] for _, row in y.iterrows()])\n",
        "  except:\n",
        "    y_encoded = y.map(name_to_id)\n",
        "\n",
        "  return y_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ffDM-mmbsl"
      },
      "source": [
        "#### **Input do modelo NPC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ettxaFkfa_nZ"
      },
      "outputs": [],
      "source": [
        "df_nextpoi_input = pd.read_csv(path_nextpoi_input_alabama)\n",
        "print(df_nextpoi_input.shape)\n",
        "\n",
        "usersids = df_nextpoi_input['userid'].unique().size\n",
        "print(usersids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu6JGPAb5gOv"
      },
      "outputs": [],
      "source": [
        "target = df_nextpoi_input.columns[900:-1]\n",
        "\n",
        "x_nextpoi = df_nextpoi_input.drop(target, axis=1)\n",
        "y_nextpoi = df_nextpoi_input[target]\n",
        "y_nextpoi = y_nextpoi.fillna('None')\n",
        "y_nextpoi = map_categories(y_nextpoi)\n",
        "y_nextpoi = pd.DataFrame(y_nextpoi)\n",
        "print(x_nextpoi.shape, y_nextpoi.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-QcXfNJ_W4B"
      },
      "source": [
        "#### **Input do modelo PCat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZPec-PIbPaj"
      },
      "outputs": [],
      "source": [
        "categorypoi_input = pd.read_csv(path_categorypoi_input_alabama)\n",
        "print(categorypoi_input.shape)\n",
        "\n",
        "placesids = categorypoi_input['placeid'].unique()\n",
        "placesids = placesids.astype(int)\n",
        "print(len(placesids))\n",
        "\n",
        "categorypoi_input = categorypoi_input.set_index('placeid')\n",
        "print(categorypoi_input.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC8O7mv86RZY"
      },
      "outputs": [],
      "source": [
        "x_categorypoi = categorypoi_input.drop('category', axis=1)\n",
        "y_categorypoi = categorypoi_input['category']\n",
        "\n",
        "y_categorypoi = map_categories(y_categorypoi)\n",
        "\n",
        "print(x_categorypoi.shape, y_categorypoi.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI_sLoXqYJn-"
      },
      "source": [
        "### **Definição dos Dataset e Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5cFxPZ2TOIe"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_features, true_y):\n",
        "        self.input_features = input_features\n",
        "        self.true_y = true_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'x': self.input_features[idx],\n",
        "            'y': self.true_y[idx]\n",
        "        }\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5kH8KMkcjS6"
      },
      "outputs": [],
      "source": [
        "def input_to_dataloader(x, y, batch_size=32, shuffle=True):\n",
        "  dataset = CustomDataset(x, y)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Otimizador**"
      ],
      "metadata": {
        "id": "L5fN4eCvR5a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa parte você só precisa adicionar o otimizador que você vai testar. Deixei a definição do PCGrad de exemplo aqui pra você mas qualquer coisa é só me chamar!"
      ],
      "metadata": {
        "id": "BLAW09vbSB-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple, Union\n",
        "from abc import abstractmethod\n",
        "\n",
        "class WeightMethod:\n",
        "    def __init__(self, n_tasks: int, device: torch.device):\n",
        "        super().__init__()\n",
        "        self.n_tasks = n_tasks\n",
        "        self.device = device\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_weighted_loss(\n",
        "        self,\n",
        "        losses: torch.Tensor,\n",
        "        shared_parameters: Union[List[torch.nn.parameter.Parameter], torch.Tensor],\n",
        "        task_specific_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ],\n",
        "        last_shared_parameters: Union[List[torch.nn.parameter.Parameter], torch.Tensor],\n",
        "        representation: Union[torch.nn.parameter.Parameter, torch.Tensor],\n",
        "        **kwargs,\n",
        "    ):\n",
        "        pass\n",
        "\n",
        "    def backward(\n",
        "        self,\n",
        "        losses: torch.Tensor,\n",
        "        shared_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        task_specific_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        last_shared_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        representation: Union[List[torch.nn.parameter.Parameter], torch.Tensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Union[torch.Tensor, None], Union[dict, None]]:\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        losses :\n",
        "        shared_parameters :\n",
        "        task_specific_parameters :\n",
        "        last_shared_parameters : parameters of last shared layer/block\n",
        "        representation : shared representation\n",
        "        kwargs :\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Loss, extra outputs\n",
        "        \"\"\"\n",
        "        loss, extra_outputs = self.get_weighted_loss(\n",
        "            losses=losses,\n",
        "            shared_parameters=shared_parameters,\n",
        "            task_specific_parameters=task_specific_parameters,\n",
        "            last_shared_parameters=last_shared_parameters,\n",
        "            representation=representation,\n",
        "            **kwargs,\n",
        "        )\n",
        "        loss.backward()\n",
        "        return loss, extra_outputs\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        losses: torch.Tensor,\n",
        "        shared_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        task_specific_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        return self.backward(\n",
        "            losses=losses,\n",
        "            shared_parameters=shared_parameters,\n",
        "            task_specific_parameters=task_specific_parameters,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def parameters(self) -> List[torch.Tensor]:\n",
        "        \"\"\"return learnable parameters\"\"\"\n",
        "        return []"
      ],
      "metadata": {
        "id": "2lWMLJgXSMjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCGrad(WeightMethod):\n",
        "    \"\"\"Modification of: https://github.com/WeiChengTseng/Pytorch-PCGrad/blob/master/pcgrad.py\n",
        "\n",
        "    @misc{Pytorch-PCGrad,\n",
        "      author = {Wei-Cheng Tseng},\n",
        "      title = {WeiChengTseng/Pytorch-PCGrad},\n",
        "      url = {https://github.com/WeiChengTseng/Pytorch-PCGrad.git},\n",
        "      year = {2020}\n",
        "    }\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_tasks: int, device: torch.device, reduction=\"sum\"):\n",
        "        super().__init__(n_tasks, device=device)\n",
        "        assert reduction in [\"mean\", \"sum\"]\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def get_weighted_loss(\n",
        "        self,\n",
        "        losses: torch.Tensor,\n",
        "        shared_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        task_specific_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _set_pc_grads(self, losses, shared_parameters, task_specific_parameters=None):\n",
        "        # shared part\n",
        "        shared_grads = []\n",
        "        for l in losses:\n",
        "            shared_grads.append(\n",
        "                torch.autograd.grad(l, shared_parameters, retain_graph=True, allow_unused=True)\n",
        "            )\n",
        "\n",
        "        if isinstance(shared_parameters, torch.Tensor):\n",
        "            shared_parameters = [shared_parameters]\n",
        "        non_conflict_shared_grads = self._project_conflicting(shared_grads)\n",
        "        for p, g in zip(shared_parameters, non_conflict_shared_grads):\n",
        "            p.grad = g\n",
        "\n",
        "        # task specific part\n",
        "        if task_specific_parameters is not None:\n",
        "            task_specific_grads = torch.autograd.grad(\n",
        "                losses.sum(), task_specific_parameters\n",
        "            )\n",
        "            if isinstance(task_specific_parameters, torch.Tensor):\n",
        "                task_specific_parameters = [task_specific_parameters]\n",
        "            for p, g in zip(task_specific_parameters, task_specific_grads):\n",
        "                p.grad = g\n",
        "\n",
        "    def _project_conflicting(self, grads: List[Tuple[torch.Tensor]]):\n",
        "        pc_grad = copy.deepcopy(grads)\n",
        "        for g_i in pc_grad:\n",
        "            random.shuffle(grads)\n",
        "            for g_j in grads:\n",
        "                g_i_g_j = sum(\n",
        "                    [\n",
        "                        torch.dot(torch.flatten(grad_i), torch.flatten(grad_j))\n",
        "                        for grad_i, grad_j in zip(g_i, g_j)\n",
        "                    ]\n",
        "                )\n",
        "                if g_i_g_j < 0:\n",
        "                    g_j_norm_square = (\n",
        "                        torch.norm(torch.cat([torch.flatten(g) for g in g_j])) ** 2\n",
        "                    )\n",
        "                    for grad_i, grad_j in zip(g_i, g_j):\n",
        "                        grad_i -= g_i_g_j * grad_j / g_j_norm_square\n",
        "\n",
        "        merged_grad = [sum(g) for g in zip(*pc_grad)]\n",
        "        if self.reduction == \"mean\":\n",
        "            merged_grad = [g / self.n_tasks for g in merged_grad]\n",
        "\n",
        "        return merged_grad\n",
        "\n",
        "    def backward(\n",
        "        self,\n",
        "        losses: torch.Tensor,\n",
        "        parameters: Union[List[torch.nn.parameter.Parameter], torch.Tensor] = None,\n",
        "        shared_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        task_specific_parameters: Union[\n",
        "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
        "        ] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self._set_pc_grads(losses, shared_parameters, task_specific_parameters)\n",
        "        return None, {}  # NOTE: to align with all other weight methods"
      ],
      "metadata": {
        "id": "tl_RsdRtSPQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfQnUuKgIUz5"
      },
      "source": [
        "### **Arquitetura do modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj_OYEQeRrrC"
      },
      "source": [
        "#### **Arquitetura do modelo NPC**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_seq_length=5000, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim: The dimensionality of the embedding space.\n",
        "            max_seq_length: The maximum sequence length to handle.\n",
        "            dropout: Dropout rate applied to positional encodings.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        positional_encoding = torch.zeros(max_seq_length, embed_dim)\n",
        "        positions = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "\n",
        "        positional_encoding[:, 0::2] = torch.sin(positions * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(positions * div_term)\n",
        "\n",
        "        positional_encoding = positional_encoding.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('positional_encoding', positional_encoding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_length, embed_dim).\n",
        "        Returns:\n",
        "            Tensor with positional encodings added to the input embeddings.\n",
        "        \"\"\"\n",
        "        seq_length = x.size(1)\n",
        "        x = x + self.positional_encoding[:, :seq_length, :]\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "ogP9pbGxvRTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8mIQCj-WeYL"
      },
      "outputs": [],
      "source": [
        "class NextPoinet(nn.Module):\n",
        "    def __init__(self, embed_dim, num_classes, num_heads, seq_length, num_layers, dropout=0.1):\n",
        "        super(NextPoinet, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.seq_length = seq_length\n",
        "        self.pe = PositionalEncoding(embed_dim, 9)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dim_feedforward=embed_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        self.linear_layers = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pe(x)\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        attn_mask = torch.triu(torch.ones(seq_length, seq_length) * float('-inf'), diagonal=1).to(x.device)\n",
        "\n",
        "        x = self.transformer_encoder(x, mask=attn_mask)\n",
        "\n",
        "        x = self.linear_layers(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuehE4PiRvfS"
      },
      "source": [
        "#### **Arquitetura do modelo PCat**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoryPoinet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CategoryPoinet, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(256, 484)\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        b, seq, dim = x.shape\n",
        "        dim = int(dim ** 0.5)\n",
        "        x = x.view(b, seq, dim, dim)\n",
        "\n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.max_pool1(out)\n",
        "\n",
        "        out = self.conv_layer3(out)\n",
        "        out = self.conv_layer4(out)\n",
        "        out = self.max_pool2(out)\n",
        "\n",
        "        out = out.view(b, seq, -1)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "zmwap_SdZk3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_-c0ilfRzdo"
      },
      "source": [
        "#### **Arquitetura do modelo MTL-POI**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MTLnet(nn.Module):\n",
        "    def __init__(self, feature_size, shared_layer_size, num_classes, num_heads, num_layers, seq_length, num_shared_layers):\n",
        "        super(MTLnet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feature_size = feature_size\n",
        "        self.embedding = torch.nn.Embedding(1, feature_size)\n",
        "\n",
        "        shared_linear_layers = [] # lista de layers compartilhadas\n",
        "\n",
        "         # add primeira camada de input (feature_size -> shared_layer_size)\n",
        "        shared_linear_layers.append(nn.Linear(feature_size, shared_layer_size))\n",
        "        shared_linear_layers.append(nn.LeakyReLU())\n",
        "        shared_linear_layers.append(nn.Dropout())\n",
        "\n",
        "        # add layers intermediarias\n",
        "        for _ in range(num_shared_layers - 1):\n",
        "            shared_linear_layers.append(nn.Linear(shared_layer_size, shared_layer_size))\n",
        "            shared_linear_layers.append(nn.LeakyReLU())\n",
        "            shared_linear_layers.append(nn.Dropout())\n",
        "\n",
        "        # cria o sequential igual antes\n",
        "        self.shared_layers = nn.Sequential(*shared_linear_layers)\n",
        "\n",
        "        self.category_poi = CategoryPoinet(num_classes)\n",
        "        self.next_poi = NextPoinet(shared_layer_size, num_classes, num_heads, seq_length, num_layers)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        idxs = x2.sum(-1) == 0\n",
        "\n",
        "        if torch.any(idxs):\n",
        "          replace_tensor = self.embedding(torch.tensor(0, dtype=torch.long).to(device))\n",
        "          x2[idxs] = replace_tensor\n",
        "\n",
        "        shared_output1 = self.shared_layers(x1)\n",
        "        shared_output2 = self.shared_layers(x2)\n",
        "\n",
        "        out1 = self.category_poi(shared_output1)\n",
        "        out2 = self.next_poi(shared_output2)\n",
        "\n",
        "        out1 = out1.view(-1, self.num_classes)\n",
        "\n",
        "        return out1, out2\n",
        "\n",
        "    def forward_nextpoi(self, x):\n",
        "        idxs = x.sum(-1) == 0\n",
        "        x[idxs] = self.embedding(torch.tensor(0,dtype=torch.long).to(device))\n",
        "\n",
        "        shared_output = self.shared_layers(x)\n",
        "\n",
        "        out = self.next_poi(shared_output)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward_categorypoi(self, x):\n",
        "        shared_output = self.shared_layers(x)\n",
        "\n",
        "        out = self.category_poi(shared_output)\n",
        "\n",
        "        out = out.view(-1, self.num_classes)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "tVzyoP6LfPWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cálculo do FLOPs do modelo**"
      ],
      "metadata": {
        "id": "pVLZGsv12hU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MTLnet(\n",
        "      feature_size=100,\n",
        "      shared_layer_size=256,\n",
        "      num_classes=7,\n",
        "      num_heads=8,\n",
        "      num_layers=4,\n",
        "      seq_length=9,\n",
        "      num_shared_layers=4,\n",
        "    )\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Pw14Pj7P2puv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_nextpoi_ = x_nextpoi.copy()\n",
        "x_nextpoi_.userid = x_nextpoi_.userid.astype(int)\n",
        "usersids = x_nextpoi_['userid'].unique()\n",
        "\n",
        "train_user_index, test_user_index = train_test_split(usersids, test_size=0.25, random_state=42)\n",
        "\n",
        "train_place_index, test_place_index = train_test_split(x_categorypoi.index, test_size=0.25, random_state=42)\n",
        "\n",
        "train_user_index, val_user_index = train_test_split(train_user_index, test_size=0.25, random_state=42)\n",
        "train_place_index, val_place_index = train_test_split(train_place_index, test_size=0.25, random_state=42)\n",
        "\n",
        "train_index_next = x_nextpoi_[x_nextpoi_['userid'].isin(train_user_index)].index\n",
        "x_nextpoi_ = x_nextpoi_.drop('userid', axis=1)\n",
        "\n",
        "y_nextpoi_ = y_nextpoi.copy()\n",
        "x_nextpoi_, y_nextpoi_ = x_y_to_tensor(x_nextpoi_, y_nextpoi_, 'next')\n",
        "x_train_next = x_nextpoi_[train_index_next]\n",
        "y_train_next = y_nextpoi_[train_index_next]\n",
        "\n",
        "x_categorypoi_ = x_categorypoi.copy()\n",
        "y_categorypoi_ = y_categorypoi.copy()\n",
        "x_categorypoi_, y_categorypoi_ = x_y_to_tensor(x_categorypoi_, y_categorypoi_, 'category')\n",
        "\n",
        "if isinstance(x_categorypoi_, torch.Tensor):\n",
        "    train_place_index, test_place_index = train_test_split(torch.arange(x_categorypoi_.size(0)), test_size=0.25, random_state=42)\n",
        "    train_place_index, val_place_index = train_test_split(train_place_index, test_size=0.25, random_state=42)\n",
        "\n",
        "    x_train_category = x_categorypoi_[train_place_index]\n",
        "    y_train_category = y_categorypoi_[train_place_index]\n",
        "\n",
        "    x_test_category = x_categorypoi_[test_place_index]\n",
        "    y_test_category = y_categorypoi_[test_place_index]\n",
        "\n",
        "    x_val_category = x_categorypoi_[val_place_index]\n",
        "    y_val_category = y_categorypoi_[val_place_index]\n",
        "else:\n",
        "    raise TypeError(\"x_categorypoi should be a tensor\")\n",
        "\n",
        "next_dataloader_train = input_to_dataloader(x_train_next, y_train_next)\n",
        "category_dataloader_train = input_to_dataloader(x_train_category, y_train_category)"
      ],
      "metadata": {
        "id": "ZMw2XJPp68mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tamanho de x_categorypoi:\", x_categorypoi.shape[0])\n",
        "print(\"train_place_index:\", train_place_index)\n",
        "print(\"test_place_index:\", test_place_index)\n",
        "print(\"val_place_index:\", val_place_index)\n",
        "print(type(x_categorypoi))"
      ],
      "metadata": {
        "id": "ZKKWaaNB92Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch_nextpoi = next(iter(next_dataloader_train))\n",
        "x1 = first_batch_nextpoi['x']\n",
        "\n",
        "first_batch_categorypoi = next(iter(category_dataloader_train))\n",
        "x2 = first_batch_categorypoi['x']\n",
        "\n",
        "x1 = x1.to(device)\n",
        "x2 = x2.to(device)"
      ],
      "metadata": {
        "id": "yxTWex855Rsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flops, macs, params = calculate_flops(model=model,\n",
        "                                          kwargs={'x1': x2, 'x2': x1},\n",
        "                                          print_results=False)\n",
        "\n",
        "print(\"MTL-Poi FLOPs: %s   MACs: %s   Params: %s\" %(flops, macs, params))\n",
        "\n",
        "flops = {\n",
        "        'flops': flops,\n",
        "        'macs': macs,\n",
        "        'params': params,\n",
        "}"
      ],
      "metadata": {
        "id": "2aPf9yDu5Pay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYRHc6pAYPJZ"
      },
      "source": [
        "### **Treinamento e teste do modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phSYs7n_kGfr"
      },
      "source": [
        "#### **Definição da função de validação**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val_model(model, loss_function, one_hot, dataloader, id_to_name, task_name):\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "  running_loss = 0.0\n",
        "  steps = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data in dataloader:\n",
        "          x, y = data['x'], data['y']\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          if task_name == 'next':\n",
        "            out = model.forward_nextpoi(x)\n",
        "\n",
        "            B, S, _ = out.shape\n",
        "            out = out.view(B * S , -1)\n",
        "            y = y.view(B * S , -1)\n",
        "\n",
        "            valid_samples = y < 7\n",
        "            expanded_mask = valid_samples.expand(-1, out.shape[1])\n",
        "\n",
        "            y = y[valid_samples]\n",
        "            out = out[expanded_mask].view(-1, 7)\n",
        "\n",
        "            predicted = torch.argmax(out, dim=-1)\n",
        "            y_true.extend(y.tolist())\n",
        "            y_pred.extend(predicted.tolist())\n",
        "\n",
        "            # y_one_hot = one_hot[y]\n",
        "            loss = loss_function(out, y)\n",
        "            running_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "          else:\n",
        "            out = model.forward_categorypoi(x)\n",
        "\n",
        "            predicted = torch.argmax(out, dim=-1)\n",
        "            y_true.extend(y.tolist())\n",
        "            y_pred.extend(predicted.tolist())\n",
        "\n",
        "            y_one_hot = one_hot[y]\n",
        "            loss = loss_function(out, y_one_hot)\n",
        "            running_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "  average_loss = running_loss / steps if steps > 0 else 0.0\n",
        "\n",
        "  print_classification_report_val(y_true, y_pred, id_to_name, task_name)\n",
        "\n",
        "  return y_true, y_pred, average_loss"
      ],
      "metadata": {
        "id": "OIRX_eG2lhu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Definição da função de teste**"
      ],
      "metadata": {
        "id": "m4tc_46wm6jT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J38Kgh95UQt"
      },
      "outputs": [],
      "source": [
        "def test_model(model, dataloader, id_to_name, task_name):\n",
        "  print(f'\\nTESTE {task_name.upper()}\\n')\n",
        "\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data in dataloader:\n",
        "          x, y = data['x'], data['y']\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          if task_name == 'next':\n",
        "            out = model.forward_nextpoi(x)\n",
        "          else:\n",
        "            out = model.forward_categorypoi(x)\n",
        "\n",
        "          predicted = torch.argmax(out, dim=-1)\n",
        "\n",
        "          valid_samples = y < 7\n",
        "\n",
        "          y = y[valid_samples]\n",
        "          predicted = predicted[valid_samples]\n",
        "\n",
        "          y_true.extend(y.view(-1).tolist())\n",
        "          y_pred.extend(predicted.view(-1).tolist())\n",
        "\n",
        "      print_classification_report_test(y_true, y_pred, id_to_name, task_name)\n",
        "      print()\n",
        "\n",
        "  return y_true, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmbkYTTqkMmv"
      },
      "source": [
        "#### **Definição da função de treino**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlQ_L4UHhUY_"
      },
      "outputs": [],
      "source": [
        "def train_model_by_fold(model, optimizer, scheduler, next_dataloader_train, category_dataloader_train, next_dataloader_val,\n",
        "                        category_dataloader_val, loss_function, one_hot, num_epochs, id_to_name, print_interval=1):\n",
        "    mtl_train_losses = []\n",
        "    next_losses = []\n",
        "    category_losses = []\n",
        "    next_val_losses = []\n",
        "    category_val_losses = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pcgrad = PCGrad(n_tasks=2, device=device, reduction=\"sum\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        next_running_loss = 0.0\n",
        "        category_running_loss = 0.0\n",
        "        max_norm = 1.0\n",
        "\n",
        "        y_true_category = []\n",
        "        y_pred_category = []\n",
        "        y_true_next = []\n",
        "        y_pred_next = []\n",
        "\n",
        "        steps = 0\n",
        "        category_iter = cycle(category_dataloader_train)\n",
        "\n",
        "        for data_next in next_dataloader_train:\n",
        "            data_category = next(category_iter)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_next, y_next = data_next['x'], data_next['y']\n",
        "\n",
        "            x_next = x_next.to(device)\n",
        "            y_next = y_next.to(device)\n",
        "\n",
        "            x_category, y_category = data_category['x'], data_category['y']\n",
        "            x_category = x_category.to(device)\n",
        "            y_category = y_category.to(device)\n",
        "            out_category, out_next = model(x_category, x_next)\n",
        "\n",
        "            category_predicted = torch.argmax(out_category, dim=-1)\n",
        "\n",
        "            y_true_category.extend(y_category.tolist())\n",
        "            y_pred_category.extend(category_predicted.tolist())\n",
        "\n",
        "            B, S, _ = out_next.shape\n",
        "\n",
        "            out_next = out_next.view(B * S , -1)\n",
        "            y_next = y_next.view(B * S , -1)\n",
        "\n",
        "            idx_valid = (y_next < num_classes).view(-1)\n",
        "            y_next = y_next[idx_valid].view(-1)\n",
        "            out_next = out_next[idx_valid]\n",
        "\n",
        "\n",
        "            next_predicted = torch.argmax(out_next, dim=-1)\n",
        "\n",
        "            y_true_next.extend(y_next.tolist())\n",
        "            y_pred_next.extend(next_predicted.view(-1).tolist())\n",
        "\n",
        "            out_next = out_next.view(-1, num_classes)\n",
        "\n",
        "\n",
        "            loss_next = loss_function(out_next, y_next)\n",
        "            loss_category = loss_function(out_category, y_category.view(-1))\n",
        "            loss = loss_next + loss_category\n",
        "\n",
        "            #aplicar pcgrad\n",
        "            loss_next.backward(retain_graph=True)\n",
        "            loss_category.backward(retain_graph=True)\n",
        "\n",
        "            pcgrad.backward(\n",
        "            losses=torch.stack([loss_next, loss_category]),\n",
        "            shared_parameters=list(model.shared_layers.parameters()),\n",
        "            # shared_parameters=list(model.shared_parameters()),\n",
        "            # task_specific_parameters=list(model.task_specific_parameters())\n",
        "            )\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            steps += 1\n",
        "            running_loss += loss.item()\n",
        "            next_running_loss += loss_next.item()\n",
        "            category_running_loss += loss_category.item()\n",
        "\n",
        "        if steps > 0:\n",
        "            if (epoch + 1) % print_interval == 0 or epoch == 0:\n",
        "                mtl_loss_by_epoch = running_loss / steps\n",
        "                next_loss_by_epoch = next_running_loss / steps\n",
        "                category_loss_by_epoch = category_running_loss / steps\n",
        "\n",
        "                mtl_train_losses.append(mtl_loss_by_epoch)\n",
        "                next_losses.append(next_loss_by_epoch)\n",
        "                category_losses.append(category_loss_by_epoch)\n",
        "                print_train_losses(epoch, num_epochs, mtl_loss_by_epoch, next_loss_by_epoch, category_loss_by_epoch)\n",
        "                print_classification_report_train(y_true_category, y_pred_category, y_true_next, y_pred_next, id_to_name)\n",
        "                print()\n",
        "                print(\"-\" * 110)\n",
        "                print()\n",
        "\n",
        "                model.eval()\n",
        "\n",
        "                # validação category\n",
        "                val_y_true_category, val_y_pred_category, category_val_loss = val_model(model, loss_function, one_hot, category_dataloader_val, id_to_name, 'category')\n",
        "                category_val_losses.append(category_val_loss)\n",
        "\n",
        "                # validação next\n",
        "                val_y_true_next, val_y_pred_next, next_val_loss = val_model(model, loss_function, one_hot, next_dataloader_val, id_to_name, 'next')\n",
        "                next_val_losses.append(next_val_loss)\n",
        "                print(\"*\" * 110)\n",
        "\n",
        "                print_val_losses(next_val_loss, category_val_loss)\n",
        "\n",
        "                scheduler.step(mtl_loss_by_epoch)\n",
        "\n",
        "    results = {'mtl_train_losses': mtl_train_losses,\n",
        "               'next_losses': next_losses,\n",
        "               'category_losses': category_losses,\n",
        "               'next_val_losses': next_val_losses,\n",
        "               'category_val_losses': category_val_losses,\n",
        "               'y_true_category': y_true_category,\n",
        "               'y_pred_category': y_pred_category,\n",
        "               'y_true_next': y_true_next,\n",
        "               'y_pred_next': y_pred_next,\n",
        "               'val_y_true_category': val_y_true_category,\n",
        "               'val_y_pred_category': val_y_pred_category,\n",
        "               'val_y_true_next': val_y_true_next,\n",
        "               'val_y_pred_next': val_y_pred_next\n",
        "               }\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g7EaBaFknkI"
      },
      "source": [
        "#### **Treinamento do modelo com validação cruzada**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_L4d_6XcHUb"
      },
      "outputs": [],
      "source": [
        "num_classes = 7\n",
        "num_epochs = 50\n",
        "fold = 1\n",
        "print_interval = 1\n",
        "learning_rate = 0.0001\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "one_hot = torch.eye(num_classes).to(device)\n",
        "\n",
        "try:\n",
        "  x_nextpoi.userid = x_nextpoi.userid.astype(int)\n",
        "  usersids = x_nextpoi['userid'].unique()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "fold_results_test = {}\n",
        "fold_results_train = {}\n",
        "fold_losses_train = {}\n",
        "fold_results_val = {}\n",
        "fold_losses_val = {}\n",
        "\n",
        "for (train_user_index, test_user_index), (train_place_index, test_place_index) in zip(kf.split(usersids), kf.split(x_categorypoi)):\n",
        "    print(\"#\" * 110)\n",
        "    print(f'FOLD {fold}:')\n",
        "\n",
        "    try:\n",
        "      x_nextpoi = _x_nextpoi.copy()\n",
        "      y_nextpoi = _y_nextpoi.copy()\n",
        "      x_categorypoi = _x_categorypoi.copy()\n",
        "      y_categorypoi = _y_categorypoi.copy()\n",
        "    except:\n",
        "      _x_nextpoi = x_nextpoi.copy()\n",
        "      _y_nextpoi = y_nextpoi.copy()\n",
        "      _x_categorypoi = x_categorypoi.copy()\n",
        "      _y_categorypoi = y_categorypoi.copy()\n",
        "\n",
        "    model = MTLnet(\n",
        "      feature_size=100,\n",
        "      shared_layer_size=256,\n",
        "      num_classes=num_classes,\n",
        "      num_heads=8,\n",
        "      num_layers=4,\n",
        "      seq_length=9,\n",
        "      num_shared_layers=4,\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "    # val e train\n",
        "    train_user_index, val_user_index = train_test_split(train_user_index, test_size=0.25, random_state=42)\n",
        "    train_place_index, val_place_index = train_test_split(train_place_index, test_size=0.25, random_state=42)\n",
        "\n",
        "    # indexs para o split pro nextpoi\n",
        "    train_index_next = x_nextpoi[x_nextpoi['userid'].isin(usersids[train_user_index])].index\n",
        "    val_index_next = x_nextpoi[x_nextpoi['userid'].isin(usersids[val_user_index])].index\n",
        "    test_index_next = x_nextpoi[x_nextpoi['userid'].isin(usersids[test_user_index])].index\n",
        "    x_nextpoi = x_nextpoi.drop('userid', axis=1)\n",
        "\n",
        "    # train, test e val pro nextpoi\n",
        "    x_nextpoi, y_nextpoi = x_y_to_tensor(x_nextpoi, y_nextpoi, 'next')\n",
        "    x_train_next, x_test_next, x_val_next = x_nextpoi[train_index_next], x_nextpoi[test_index_next], x_nextpoi[val_index_next]\n",
        "    y_train_next, y_test_next, y_val_next = y_nextpoi[train_index_next], y_nextpoi[test_index_next], y_nextpoi[val_index_next]\n",
        "\n",
        "    # train, test e val pro categorypoi\n",
        "    x_categorypoi, y_categorypoi = x_y_to_tensor(x_categorypoi, y_categorypoi, 'category')\n",
        "    x_train_category, x_test_category, x_val_category = x_categorypoi[train_place_index], x_categorypoi[test_place_index], x_categorypoi[val_place_index]\n",
        "    y_train_category, y_test_category, y_val_category = y_categorypoi[train_place_index], y_categorypoi[test_place_index], y_categorypoi[val_place_index]\n",
        "\n",
        "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_category.cpu().numpy()), y=y_train_category.cpu().numpy())\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    loss_function_PCatEncoder = nn.MSELoss()\n",
        "\n",
        "    # dataloaders nextpoi\n",
        "    next_dataloader_train = input_to_dataloader(x_train_next, y_train_next)\n",
        "    next_dataloader_test = input_to_dataloader(x_test_next, y_test_next)\n",
        "    next_dataloader_val = input_to_dataloader(x_val_next, y_val_next)\n",
        "\n",
        "    # dataloaders categorypoi\n",
        "    category_dataloader_train = input_to_dataloader(x_train_category, y_train_category)\n",
        "    category_dataloader_test = input_to_dataloader(x_test_category, y_test_category)\n",
        "    category_dataloader_val = input_to_dataloader(x_val_category, y_val_category)\n",
        "\n",
        "    fold_results_test[fold] = {\n",
        "        'test_next_true': [],\n",
        "        'test_next_pred': [],\n",
        "        'test_category_true': [],\n",
        "        'test_category_pred': []\n",
        "    }\n",
        "\n",
        "    fold_results_train[fold] = {\n",
        "        'train_next_true': [],\n",
        "        'train_next_pred': [],\n",
        "        'train_category_true': [],\n",
        "        'train_category_pred': [],\n",
        "    }\n",
        "\n",
        "    fold_results_val[fold] = {\n",
        "        'val_next_true': [],\n",
        "        'val_next_pred': [],\n",
        "        'val_category_true': [],\n",
        "        'val_category_pred': [],\n",
        "    }\n",
        "\n",
        "    fold_losses_train[fold] = {\n",
        "        'mtl_train_losses': [],\n",
        "        'next_losses': [],\n",
        "        'category_losses': []\n",
        "    }\n",
        "\n",
        "    fold_losses_val[fold] = {\n",
        "        'next_val_losses': [],\n",
        "        'category_val_losses': [],\n",
        "    }\n",
        "\n",
        "    # train mtl model\n",
        "    results = train_model_by_fold(model, optimizer, scheduler, next_dataloader_train, category_dataloader_train,\n",
        "                                  next_dataloader_val, category_dataloader_val,\n",
        "                                  loss_function, loss_function_PCatEncoder, one_hot, num_epochs, id_to_name)\n",
        "\n",
        "    fold_losses_train[fold]['mtl_train_losses'] = results['mtl_train_losses']\n",
        "    fold_losses_train[fold]['next_losses'] = results['next_losses']\n",
        "    fold_losses_train[fold]['category_losses'] = results['category_losses']\n",
        "\n",
        "    fold_losses_val[fold]['next_val_losses'] = results['next_val_losses']\n",
        "    fold_losses_val[fold]['category_val_losses'] = results['category_val_losses']\n",
        "\n",
        "    fold_results_train[fold]['train_next_true'] = results['y_true_next']\n",
        "    fold_results_train[fold]['train_next_pred'] = results['y_pred_next']\n",
        "    fold_results_train[fold]['train_category_true'] = results['y_true_category']\n",
        "    fold_results_train[fold]['train_category_pred'] = results['y_pred_category']\n",
        "\n",
        "    fold_results_val[fold]['val_next_true'] = results['val_y_true_next']\n",
        "    fold_results_val[fold]['val_next_pred'] = results['val_y_pred_next']\n",
        "    fold_results_val[fold]['val_category_true'] = results['val_y_true_category']\n",
        "    fold_results_val[fold]['val_category_pred'] = results['val_y_pred_category']\n",
        "\n",
        "    # test next\n",
        "    y_true_test_next, y_pred_test_next = test_model(model, next_dataloader_test, id_to_name, 'next')\n",
        "    fold_results_test[fold]['test_next_true'] = y_true_test_next\n",
        "    fold_results_test[fold]['test_next_pred'] = y_pred_test_next\n",
        "\n",
        "    # test category\n",
        "    y_true_test_category, y_pred_test_category = test_model(model, category_dataloader_test, id_to_name, 'category')\n",
        "    fold_results_test[fold]['test_category_true'] = y_true_test_category\n",
        "    fold_results_test[fold]['test_category_pred'] = y_pred_test_category\n",
        "\n",
        "    fold += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWHWE0bArD_n"
      },
      "source": [
        "### **Análise dos resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUZ_NEU8suQQ"
      },
      "source": [
        "#### **Distribuição das métricas de teste**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRmgRkUidg3u"
      },
      "source": [
        "##### **NPC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snGd_wglMbRO"
      },
      "outputs": [],
      "source": [
        "metrics_df_next = calculate_metrics_by_fold(fold_results_test, 'next')\n",
        "print('nextpoi test metrics by fold:\\n')\n",
        "print(metrics_df_next)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkZZD5zsdDyN"
      },
      "outputs": [],
      "source": [
        "metrics = ['precision', 'recall', 'f1-score']\n",
        "cores = ['green', 'purple', 'yellow']\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    sns.boxplot(y=metric, data=metrics_df_next, color=cores[i])\n",
        "    plt.title(f'{metric.capitalize()} por Fold')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "\n",
        "    min_val = metrics_df_next[metric].min() * 0.95\n",
        "    max_val = metrics_df_next[metric].max() * 1.05\n",
        "    plt.ylim(min_val, max_val)\n",
        "\n",
        "    plt.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "metric_by_fold_nextpoi = '/tmp/metric-by-fold-nextpoi.png'\n",
        "plt.savefig(metric_by_fold_nextpoi)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWI9E5WId1iY"
      },
      "source": [
        "##### **PCat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp5wZM7Wd4Ku"
      },
      "outputs": [],
      "source": [
        "metrics_df_category = calculate_metrics_by_fold(fold_results_test, 'category')\n",
        "print('categorypoi test metrics by fold:\\n')\n",
        "print(metrics_df_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83ikY4i_d6A4"
      },
      "outputs": [],
      "source": [
        "metrics = ['precision', 'recall', 'f1-score']\n",
        "cores = ['green', 'purple', 'yellow']\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    sns.boxplot(y=metric, data=metrics_df_category, color=cores[i])\n",
        "    plt.title(f'{metric.capitalize()} por Fold')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "\n",
        "    min_val = metrics_df_category[metric].min() * 0.95\n",
        "    max_val = metrics_df_category[metric].max() * 1.05\n",
        "    plt.ylim(min_val, max_val)\n",
        "\n",
        "    plt.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "metric_by_fold_categorypoi = '/tmp/metric-by-fold-categorypoi.png'\n",
        "plt.savefig(metric_by_fold_categorypoi)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSHcFFoXkZVH"
      },
      "source": [
        "#### **Métricas de teste em cada fold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ADvJnDAR99d"
      },
      "outputs": [],
      "source": [
        "for fold in fold_results_test:\n",
        "  print('\"' * 85)\n",
        "  print(f'FOLD {fold}\\n')\n",
        "\n",
        "  y_true_next = fold_results_test[fold]['test_next_true']\n",
        "  y_pred_next = fold_results_test[fold]['test_next_pred']\n",
        "\n",
        "  y_true_category = fold_results_test[fold]['test_category_true']\n",
        "  y_pred_category = fold_results_test[fold]['test_category_pred']\n",
        "\n",
        "  print_classification_report_test(y_true_next, y_pred_next, id_to_name, 'next')\n",
        "  print_classification_report_test(y_true_category, y_pred_category, id_to_name, 'category')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqatGKkhTeEr"
      },
      "source": [
        "#### **Evolução das perdas**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Durante o treinamento**"
      ],
      "metadata": {
        "id": "2gY6Gq0lC-Vo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjwnNiNJ_lXa"
      },
      "outputs": [],
      "source": [
        "num_folds = len(fold_losses_train)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for fold in range(1, num_folds + 1):\n",
        "    epochs = range(1, len(fold_losses_train[fold]['next_losses']) + 1)\n",
        "    losses = fold_losses_train[fold]['next_losses']\n",
        "    plt.plot(epochs, losses, marker='o', label=f'Fold {fold}')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Evolução das Perdas no Treino por Fold (NPC)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "next_train_loss_graph = \"/tmp/loss-next-by-fold-.png\"\n",
        "plt.savefig(next_train_loss_graph)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds = len(fold_losses_train)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for fold in range(1, num_folds + 1):\n",
        "    epochs = range(1, len(fold_losses_train[fold]['category_losses']) + 1)\n",
        "    losses = fold_losses_train[fold]['category_losses']\n",
        "    plt.plot(epochs, losses, marker='o', label=f'Fold {fold}')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Evolução das Perdas no Treino por Fold (PCat)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "cat_train_loss_graph = \"/tmp/loss-category-by-fold-.png\"\n",
        "plt.savefig(cat_train_loss_graph)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ETHkqC77Q6Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Durante a validação**"
      ],
      "metadata": {
        "id": "YA7Vqy_vQnBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds = len(fold_losses_train)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for fold in range(1, num_folds + 1):\n",
        "    epochs = range(1, len(fold_losses_val[fold]['next_val_losses']) + 1)\n",
        "    losses = fold_losses_val[fold]['next_val_losses']\n",
        "    plt.plot(epochs, losses, marker='o', label=f'Fold {fold}')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Evolução das Perdas na Validação por Fold (NPC)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "next_val_loss_graph = \"/tmp/next_val_loss_graph.png\"\n",
        "plt.savefig(next_val_loss_graph)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MCX_Ki08QomY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds = len(fold_losses_train)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for fold in range(1, num_folds + 1):\n",
        "    epochs = range(1, len(fold_losses_val[fold]['category_val_losses']) + 1)\n",
        "    losses = fold_losses_val[fold]['category_val_losses']\n",
        "    plt.plot(epochs, losses, marker='o', label=f'Fold {fold}')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Evolução das Perdas na Validação por Fold (PCat)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "cat_val_loss_graph = \"/tmp/category_val_loss_graph.png\"\n",
        "plt.savefig(cat_val_loss_graph)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mn-Wz0NeQ-Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EgwBoTR_t6k"
      },
      "source": [
        "### **Salvar resultados no drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4kzHmRy_y-9"
      },
      "outputs": [],
      "source": [
        "model_name_next = 'NPC'\n",
        "model_name_category = 'PCat'\n",
        "state = 'alabama'\n",
        "\n",
        "def _classification_report(y_true, y_pred, id_to_name, fold_num):\n",
        "    report = classification_report(y_true, y_pred, zero_division=1, output_dict=True)\n",
        "    metrics_df = pd.DataFrame(report).transpose()\n",
        "    metrics_df = metrics_df.drop(columns='support')\n",
        "    metrics_df.rename(index=id_to_name, inplace=True)\n",
        "    metrics_df = metrics_df.transpose()\n",
        "    metrics_df = metrics_df.map(lambda x: f\"{x * 100:.1f}\")\n",
        "    accuracy = f\"{report['accuracy'] * 100:.1f}\"\n",
        "\n",
        "    return metrics_df, accuracy\n",
        "\n",
        "agg_metrics_list_next = []\n",
        "metrics_list_next = []\n",
        "\n",
        "agg_metrics_list_category = []\n",
        "metrics_list_category = []\n",
        "\n",
        "for fold in fold_results_test:\n",
        "    y_true_next = fold_results_test[fold]['test_next_true']\n",
        "    y_pred_next = fold_results_test[fold]['test_next_pred']\n",
        "    report_next, accuracy_next = _classification_report(y_true_next, y_pred_next, id_to_name, fold)\n",
        "\n",
        "    y_true_category = fold_results_test[fold]['test_category_true']\n",
        "    y_pred_category = fold_results_test[fold]['test_category_pred']\n",
        "    report_category, accuracy_category = _classification_report(y_true_category, y_pred_category, id_to_name, fold)\n",
        "\n",
        "    df_metrics_next = report_next.copy()\n",
        "    df_metrics_next.reset_index(inplace=True)\n",
        "    df_metrics_next = df_metrics_next.rename(columns={'index': 'Metricas'})\n",
        "    path_df = f'/tmp/next-metrics-fold{fold}.csv'\n",
        "    df_metrics_next.to_csv(path_df)\n",
        "\n",
        "    df_metricscategory = report_category.copy()\n",
        "    df_metricscategory.reset_index(inplace=True)\n",
        "    df_metricscategory = df_metricscategory.rename(columns={'index': 'Metricas'})\n",
        "    path_df = f'/tmp/category-metrics-fold{fold}.csv'\n",
        "    df_metricscategory.to_csv(path_df)\n",
        "\n",
        "    # primeiro csv que gera um grafico\n",
        "    agg_metrics_next = {\n",
        "        'fold': fold,\n",
        "        'model': model_name_next,\n",
        "        'accuracy': accuracy_next,\n",
        "        'macro avg': report_next.loc['f1-score', 'macro avg'],\n",
        "        'weighted avg': report_next.loc['f1-score', 'weighted avg'],\n",
        "        'state': state\n",
        "    }\n",
        "    agg_metrics_list_next.append(agg_metrics_next)\n",
        "\n",
        "    agg_metrics_category = {\n",
        "        'fold': fold,\n",
        "        'model': model_name_category,\n",
        "        'accuracy': accuracy_category,\n",
        "        'macro avg': report_category.loc['f1-score', 'macro avg'],\n",
        "        'weighted avg': report_category.loc['f1-score', 'weighted avg'],\n",
        "        'state': state\n",
        "    }\n",
        "    agg_metrics_list_category.append(agg_metrics_category)\n",
        "\n",
        "    # segundo csv que gera a tabela\n",
        "    for categoria in report_next.columns:\n",
        "        if categoria not in ['macro avg', 'weighted avg']:\n",
        "            next_metrics = {\n",
        "                'precision': report_next.loc['precision', categoria],\n",
        "                'recall': report_next.loc['recall', categoria],\n",
        "                'f1-score': report_next.loc['f1-score', categoria],\n",
        "                'category': categoria,\n",
        "                'accuracy': accuracy_next,\n",
        "                'model': model_name_next,\n",
        "                'state': state\n",
        "            }\n",
        "            metrics_list_next.append(next_metrics)\n",
        "\n",
        "    for categoria in report_category.columns:\n",
        "        if categoria not in ['macro avg', 'weighted avg']:\n",
        "            next_metrics = {\n",
        "                'precision': report_category.loc['precision', categoria],\n",
        "                'recall': report_category.loc['recall', categoria],\n",
        "                'f1-score': report_category.loc['f1-score', categoria],\n",
        "                'category': categoria,\n",
        "                'accuracy': accuracy_category,\n",
        "                'model': model_name_category,\n",
        "                'state': state\n",
        "            }\n",
        "            metrics_list_category.append(next_metrics)\n",
        "\n",
        "agg_metrics_df_next = pd.DataFrame(agg_metrics_list_next)\n",
        "category_metrics_next = pd.DataFrame(metrics_list_next)\n",
        "\n",
        "agg_metrics_df_category = pd.DataFrame(agg_metrics_list_category)\n",
        "category_metrics_category = pd.DataFrame(metrics_list_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVMWWqlT_zkp"
      },
      "outputs": [],
      "source": [
        "path_agg_metrics_next = f'/content/drive/MyDrive/Graduacao/POC/Dados/models-results/MTLPOIPCGrad/{state}/NPC-general.csv'\n",
        "path_category_metrics_next = f'/content/drive/MyDrive/Graduacao/POC/Dados/models-results/MTLPOIPCGrad/{state}/NPC-all-metrics.csv'\n",
        "\n",
        "path_agg_metrics_category = f'/content/drive/MyDrive/Graduacao/POC/Dados/models-results/MTLPOIPCGrad/{state}/PCat-general.csv'\n",
        "path_category_metrics_category = f'/content/drive/MyDrive/Graduacao/POC/Dados/models-results/MTLPOIPCGrad/{state}/PCat-all-metrics.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do6_QPKo_1Qo"
      },
      "outputs": [],
      "source": [
        "agg_metrics_df_next.to_csv(path_agg_metrics_next, index=False)\n",
        "category_metrics_next.to_csv(path_category_metrics_next, index=False)\n",
        "\n",
        "agg_metrics_df_category.to_csv(path_agg_metrics_category, index=False)\n",
        "category_metrics_category.to_csv(path_category_metrics_category, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91fyPtm9_2gh"
      },
      "outputs": [],
      "source": [
        "model_and_train_params = {\n",
        "  'input_dim': 100,\n",
        "  'shared_layer_size': 256,\n",
        "  'num_classes': 7,\n",
        "  'num_layers': 4,\n",
        "  'num_heads': 8,\n",
        "  'seq_length': 9,\n",
        "  'lr': 0.0001,\n",
        "  'num_epochs': 100,\n",
        "  'n_splits': 5,\n",
        "  'weight_decay': 1e-5,\n",
        "  'num_shared_layers': 4,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = ['next', 'category']\n",
        "num_folds = 5\n",
        "epochs = 50\n",
        "model_name = 'MTLPOI'\n",
        "\n",
        "for task in tasks:\n",
        "    task_losses_train_avg = []\n",
        "    task_losses_val_avg = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_train_losses = []\n",
        "        epoch_val_losses = []\n",
        "\n",
        "        for fold in range(num_folds):\n",
        "            epoch_train_losses.append(fold_losses_train[fold + 1][f'{task}_losses'][epoch])\n",
        "            epoch_val_losses.append(fold_losses_val[fold + 1][f'{task}_val_losses'][epoch])\n",
        "\n",
        "        task_losses_train_avg.append(np.mean(epoch_train_losses))\n",
        "        task_losses_val_avg.append(np.mean(epoch_val_losses))\n",
        "\n",
        "    df_losses = pd.DataFrame({\n",
        "        'Epoch': range(1, epochs + 1),\n",
        "        f'{task}_train_loss_avg': task_losses_train_avg,\n",
        "        f'{task}_val_loss_avg': task_losses_val_avg\n",
        "    })\n",
        "\n",
        "    state = 'alabama'\n",
        "    task_loss_path = f\"/content/drive/MyDrive/Graduacao/POC/GerarResultados/ablation_study/{model_name}_losses_{task}_avg_{state}.csv\"\n",
        "    df_losses.to_csv(task_loss_path, index=False)\n",
        "\n",
        "    print(f\"Arquivo CSV gerado para a tarefa {task} e modelo {model_name}: {task_loss_path}\")"
      ],
      "metadata": {
        "id": "i1UdETERhak_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Salvar resultados no MlFlow**"
      ],
      "metadata": {
        "id": "WKKwW0vT2Mer"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKTmaFQa_3_4"
      },
      "outputs": [],
      "source": [
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "with mlflow.start_run(run_name='MTL-POI PCGrad'):\n",
        "    mlflow.log_params(model_and_train_params)\n",
        "    torch.save(model.state_dict(), \"mtlpoi.pth\")\n",
        "    mlflow.log_artifact(\"mtlpoi.pth\", \"Modelos\")\n",
        "    mlflow.log_artifact(next_train_loss_graph, \"Perdas de Treino\")\n",
        "    mlflow.log_artifact(cat_train_loss_graph, \"Perdas de Treino\")\n",
        "    mlflow.log_artifact(next_val_loss_graph, \"Perdas de Validação\")\n",
        "    mlflow.log_artifact(cat_val_loss_graph, \"Perdas de Validação\")\n",
        "    mlflow.log_artifact(metric_by_fold_nextpoi, \"Distribuição das Métricas de Teste\")\n",
        "    mlflow.log_artifact(metric_by_fold_categorypoi, \"Distribuição das Métricas de Teste\")\n",
        "    mlflow.log_artifact(path_agg_metrics_next, \"Métrica F1-Score (NPC)\")\n",
        "    mlflow.log_artifact(path_agg_metrics_category, \"Métrica F1-Score (PCat)\")\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        metrics_df_next_path = os.path.join(tmpdir, \"metrics_df_next.csv\")\n",
        "        metrics_df_category_path = os.path.join(tmpdir, \"metrics_df_category.csv\")\n",
        "\n",
        "        metrics_df_next.to_csv(metrics_df_next_path, index=False)\n",
        "        metrics_df_category.to_csv(metrics_df_category_path, index=False)\n",
        "\n",
        "        mlflow.log_artifact(metrics_df_next_path, \"Métricas de Teste (NPC)\")\n",
        "        mlflow.log_artifact(metrics_df_category_path, \"Métricas de Teste (NPC)\")\n",
        "\n",
        "        for fold in fold_results_test:\n",
        "            path_df_next = f'/tmp/next-metrics-fold{fold}.csv'\n",
        "            mlflow.log_artifact(path_df_next, \"Métricas de Teste Por Fold (NPC)\")\n",
        "\n",
        "            path_df_category = f'/tmp/category-metrics-fold{fold}.csv'\n",
        "            mlflow.log_artifact(path_df_category, \"Métricas de Teste por Fold (PCat)\")"
      ],
      "metadata": {
        "id": "intpxNg9GxXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4JkJj-fPIG27",
        "mDlyO8AOeSOY",
        "uYPLa-jzqxwm",
        "Vfx1F_2aq9Rc",
        "MfQnUuKgIUz5",
        "phSYs7n_kGfr",
        "m4tc_46wm6jT",
        "DmbkYTTqkMmv",
        "MRmgRkUidg3u",
        "ZWI9E5WId1iY"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}