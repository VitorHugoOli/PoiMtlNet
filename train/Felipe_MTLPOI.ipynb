{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "O modelo Multi-Task Learning é composto por uma ou mais tarefas que são treinadas de forma conjunta. O modelo aqui desenvolvido é o MTL-POI e são treinadas duas tarefas de forma conjunta, ou seja, dois modelos Single-Task:\n",
    "\n",
    "*   NPC -> NextPoiCat\n",
    "*   PCat -> PoiCategorization"
   ],
   "metadata": {
    "id": "DvNsIwtFO2oz"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JkJj-fPIG27"
   },
   "source": [
    "### **Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6pnPFkaeoh-L",
    "ExecuteTime": {
     "end_time": "2025-03-08T16:11:47.820419Z",
     "start_time": "2025-03-08T16:11:47.793972Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from typing import Iterator\n",
    "\n",
    "from calflops import calculate_flops\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import ast\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgeopandas\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pandas'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Configurações**"
   ],
   "metadata": {
    "id": "mDlyO8AOeSOY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0CThKCj4Kee",
    "outputId": "db50bfa5-5e20-4192-95ab-8dcbf1527f64"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Treinamento usando GPU.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Treinamento usando GPU.\")\n",
    "else:\n",
    "    print(\"Treinamento usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "seed = 18\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "id": "xmf1Y_7adwRY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg4qrVHuNSBA"
   },
   "source": [
    "#### **Definição dos *paths* das entradas**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo você precisa substituir com o caminho do seu drive que contêm os dados que te mandei."
   ],
   "metadata": {
    "id": "VmwJTWsfcido"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1ela_A1NWAN"
   },
   "outputs": [],
   "source": [
    "# alabama\n",
    "path_nextpoi_input_alabama = '/content/drive/MyDrive/felipe/alabama/nextpoi-input.csv'\n",
    "path_categorypoi_input_alabama = '.../felipe/alabama/categorypoi-input.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNpjU-2M4SZq"
   },
   "source": [
    "#### **Inicialização do MlFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "li9lectC4VjK",
    "outputId": "ce3a5b61-7c8c-4572-eced-d3127a6f4927"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-2.20.3-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting mlflow-skinny==2.20.3 (from mlflow)\n",
      "  Downloading mlflow_skinny-2.20.3-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (17.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.38)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.20.3->mlflow)\n",
      "  Downloading databricks_sdk-0.44.1-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (8.6.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (1.16.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (1.16.0)\n",
      "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (24.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (4.25.6)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (2.10.6)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.3->mlflow) (4.12.2)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (2.27.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.3->mlflow) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.2.18)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (75.1.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (0.37b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.3->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.3->mlflow) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (2025.1.31)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.17.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.6.1)\n",
      "Downloading mlflow-2.20.3-py3-none-any.whl (28.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m28.4/28.4 MB\u001B[0m \u001B[31m54.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading mlflow_skinny-2.20.3-py3-none-any.whl (6.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.0/6.0 MB\u001B[0m \u001B[31m87.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m233.6/233.6 kB\u001B[0m \u001B[31m22.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m147.8/147.8 kB\u001B[0m \u001B[31m15.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m114.9/114.9 kB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.0/85.0 kB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading databricks_sdk-0.44.1-py3-none-any.whl (648 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m648.7/648.7 kB\u001B[0m \u001B[31m48.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m203.4/203.4 kB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: Mako, gunicorn, graphql-core, graphql-relay, docker, alembic, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
      "Successfully installed Mako-1.3.9 alembic-1.14.1 databricks-sdk-0.44.1 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.20.3 mlflow-skinny-2.20.3\n"
     ]
    }
   ],
   "source": "import mlflow"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1-3fiqK4X_j",
    "outputId": "2424f2e5-8a02-4b4f-d52c-7df47db6af64"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/014b7128182d4764a26d95cb9bec1648', creation_time=1717415062165, experiment_id='1', last_update_time=1717415062165, lifecycle_stage='active', name='alabama', tags={}>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "DAGSHUB_USER = \"ingredalmeida1\"\n",
    "DAGSHUB_REPO = \"poc\"\n",
    "DAGSHUB_TOKEN = \"fcd26b772e42ea1a5e95651a69fb0adb7bd00e2e\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = DAGSHUB_USER\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = DAGSHUB_TOKEN\n",
    "DAGSHUB_URL = f\"https://dagshub.com/{DAGSHUB_USER}/{DAGSHUB_REPO}.mlflow\"\n",
    "mlflow.set_tracking_uri(DAGSHUB_URL)\n",
    "mlflow.set_experiment(\"alabama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY3646SHTtkl"
   },
   "source": [
    "### **Funções utilitárias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkaE_93wjEly"
   },
   "source": [
    "#### **Imprimir métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NXny8jvzJ_t"
   },
   "outputs": [],
   "source": [
    "id_to_name = {\n",
    "    '0': 'Community',\n",
    "    '1': 'Entertainment',\n",
    "    '2': 'Food',\n",
    "    '3': 'Nightlife',\n",
    "    '4': 'Outdoors',\n",
    "    '5': 'Shopping',\n",
    "    '6': 'Travel'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLTMDsh4zQ2B"
   },
   "outputs": [],
   "source": [
    "def print_classification_report_train(y_true_category, y_pred_category, y_true_next, y_pred_next, id_to_name):\n",
    "    category_report = classification_report(y_true_category, y_pred_category, zero_division=1, output_dict=True)\n",
    "    next_report = classification_report(y_true_next, y_pred_next, zero_division=1, output_dict=True)\n",
    "\n",
    "    category_df = pd.DataFrame(category_report).transpose()\n",
    "    next_df = pd.DataFrame(next_report).transpose()\n",
    "\n",
    "    category_df = category_df.drop(columns='support')\n",
    "    category_df = category_df.drop(index=['weighted avg', 'accuracy'])\n",
    "\n",
    "    next_df = next_df.drop(columns='support')\n",
    "    next_df = next_df.drop(index=['weighted avg', 'accuracy'])\n",
    "\n",
    "    category_df.rename(index=id_to_name, inplace=True)\n",
    "    next_df.rename(index=id_to_name, inplace=True)\n",
    "\n",
    "    category_df = category_df.transpose()\n",
    "    next_df = next_df.transpose()\n",
    "\n",
    "    category_df = category_df.map(lambda x: f\"{x * 100:.1f}\")\n",
    "    next_df = next_df.map(lambda x: f\"{x * 100:.1f}\")\n",
    "\n",
    "    print(\"\\ncategory train metrics:\")\n",
    "    print(category_df.to_string())\n",
    "\n",
    "    print(\"\\n next train metrics:\")\n",
    "    print(next_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSSAwExE6uQd"
   },
   "outputs": [],
   "source": [
    "def print_classification_report_test(y_true, y_pred, id_to_name, task_name):\n",
    "    report = classification_report(y_true, y_pred, zero_division=1, output_dict=True)\n",
    "\n",
    "    metrics_df = pd.DataFrame(report).transpose()\n",
    "    metrics_df = metrics_df.drop(columns='support')\n",
    "    metrics_df = metrics_df.drop(index=['weighted avg', 'accuracy'])\n",
    "\n",
    "    metrics_df.rename(index=id_to_name, inplace=True)\n",
    "    metrics_df = metrics_df.transpose()\n",
    "\n",
    "    metrics_df = metrics_df.map(lambda x: f\"{x * 100:.1f}\")\n",
    "\n",
    "    print(f'{task_name} test metrics:')\n",
    "    print(metrics_df.to_string() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2ICHVcM8-oB"
   },
   "outputs": [],
   "source": [
    "def print_classification_report_val(y_true, y_pred, id_to_name, task_name):\n",
    "    report = classification_report(y_true, y_pred, zero_division=1, output_dict=True)\n",
    "\n",
    "    metrics_df = pd.DataFrame(report).transpose()\n",
    "    metrics_df = metrics_df.drop(columns='support')\n",
    "    metrics_df = metrics_df.drop(index=['weighted avg', 'accuracy'])\n",
    "\n",
    "    metrics_df.rename(index=id_to_name, inplace=True)\n",
    "    metrics_df = metrics_df.transpose()\n",
    "\n",
    "    metrics_df = metrics_df.map(lambda x: f\"{x * 100:.1f}\")\n",
    "\n",
    "    print(f'validação {task_name} metrics:')\n",
    "    print(metrics_df.to_string() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1FiLVSsj_Y7"
   },
   "source": [
    "#### **Imprimir perdas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AG_iTubLkByI"
   },
   "outputs": [],
   "source": [
    "def print_train_losses(epoch, num_epochs, mtl_loss, next_loss, category_losss):\n",
    "  print(f'\\nEPOCH {epoch + 1}/{num_epochs}:')\n",
    "  print(f'mtl loss: {mtl_loss:.1f}')\n",
    "  print(f'next loss: {next_loss:.1f}')\n",
    "  print(f'category loss: {category_losss:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def print_val_losses(next_loss, category_losss):\n",
    "  print(f'\\nVALIDATION LOSSES:')\n",
    "  print(f'next val loss: {next_loss:.1f}')\n",
    "  print(f'category val loss: {category_losss:.1f}\\n')"
   ],
   "metadata": {
    "id": "PbV_yGTYqMiF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vfx1F_2aq9Rc"
   },
   "source": [
    "#### **Cálculo das métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxFFg2-pLCOh"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics_by_fold(fold_results, task_name):\n",
    "    metrics = {\n",
    "        'fold': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1-score': []\n",
    "    }\n",
    "\n",
    "    for fold, results in fold_results.items():\n",
    "        y_true = results[f'test_{task_name}_true']\n",
    "        y_pred = results[f'test_{task_name}_pred']\n",
    "\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n",
    "\n",
    "        metrics['fold'].append(fold)\n",
    "        metrics['precision'].append(report['macro avg']['precision'])\n",
    "        metrics['recall'].append(report['macro avg']['recall'])\n",
    "        metrics['f1-score'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77IpbaZOSo-F"
   },
   "source": [
    "### **Input do modelo MTL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bh3d55fg5kgX"
   },
   "outputs": [],
   "source": [
    "def x_y_to_tensor(x, y, task_name):\n",
    "  x = torch.tensor(x.values, dtype=torch.float)\n",
    "  y = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "  if task_name == 'next':\n",
    "     x = x.view(-1, 9, 100)\n",
    "  else:\n",
    "     x = x.view(-1, 1, 100)\n",
    "\n",
    "\n",
    "  x.to(device)\n",
    "  y.to(device)\n",
    "\n",
    "  return x, y\n",
    "\n",
    "def map_categories(y):\n",
    "  id_to_name = {\n",
    "    '0': 'Community',\n",
    "    '1': 'Entertainment',\n",
    "    '2': 'Food',\n",
    "    '3': 'Nightlife',\n",
    "    '4': 'Outdoors',\n",
    "    '5': 'Shopping',\n",
    "    '6': 'Travel',\n",
    "    '7': 'None'\n",
    "  }\n",
    "\n",
    "  name_to_id = {name: int(id) for id, name in id_to_name.items()}\n",
    "  try:\n",
    "    y_encoded = np.array([[name_to_id[yi] for yi in row] for _, row in y.iterrows()])\n",
    "  except:\n",
    "    y_encoded = y.map(name_to_id)\n",
    "\n",
    "  return y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3ffDM-mmbsl"
   },
   "source": [
    "#### **Input do modelo NPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "ettxaFkfa_nZ",
    "outputId": "a24ca9a5-53e7-4b44-f20c-c4672536e0e4"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/felipe/alabama/nextpoi-input.csv'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-605bd3ef1b4a>\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_nextpoi_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_nextpoi_input_alabama\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_nextpoi_input\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0musersids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_nextpoi_input\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'userid'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munique\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0musersids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1024\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1025\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1026\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1027\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1028\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    618\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    619\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 620\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    621\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    622\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1618\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1619\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1620\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1621\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1622\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1879\u001B[0m                     \u001B[0mmode\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m\"b\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1880\u001B[0;31m             self.handles = get_handle(\n\u001B[0m\u001B[1;32m   1881\u001B[0m                 \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1882\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    871\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    872\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 873\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    874\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    875\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/felipe/alabama/nextpoi-input.csv'"
     ]
    }
   ],
   "source": [
    "df_nextpoi_input = pd.read_csv(path_nextpoi_input_alabama)\n",
    "print(df_nextpoi_input.shape)\n",
    "\n",
    "usersids = df_nextpoi_input['userid'].unique().size\n",
    "print(usersids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wu6JGPAb5gOv"
   },
   "outputs": [],
   "source": [
    "target = df_nextpoi_input.columns[900:-1]\n",
    "\n",
    "x_nextpoi = df_nextpoi_input.drop(target, axis=1)\n",
    "y_nextpoi = df_nextpoi_input[target]\n",
    "y_nextpoi = y_nextpoi.fillna('None')\n",
    "y_nextpoi = map_categories(y_nextpoi)\n",
    "y_nextpoi = pd.DataFrame(y_nextpoi)\n",
    "print(x_nextpoi.shape, y_nextpoi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-QcXfNJ_W4B"
   },
   "source": [
    "#### **Input do modelo PCat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZPec-PIbPaj"
   },
   "outputs": [],
   "source": [
    "categorypoi_input = pd.read_csv(path_categorypoi_input_alabama)\n",
    "print(categorypoi_input.shape)\n",
    "\n",
    "placesids = categorypoi_input['placeid'].unique()\n",
    "placesids = placesids.astype(int)\n",
    "print(len(placesids))\n",
    "\n",
    "categorypoi_input = categorypoi_input.set_index('placeid')\n",
    "print(categorypoi_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC8O7mv86RZY"
   },
   "outputs": [],
   "source": [
    "x_categorypoi = categorypoi_input.drop('category', axis=1)\n",
    "y_categorypoi = categorypoi_input['category']\n",
    "\n",
    "y_categorypoi = map_categories(y_categorypoi)\n",
    "\n",
    "print(x_categorypoi.shape, y_categorypoi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI_sLoXqYJn-"
   },
   "source": [
    "### **Definição dos Dataset e Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5cFxPZ2TOIe"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_features, true_y):\n",
    "        self.input_features = input_features\n",
    "        self.true_y = true_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'x': self.input_features[idx],\n",
    "            'y': self.true_y[idx]\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5kH8KMkcjS6"
   },
   "outputs": [],
   "source": [
    "def input_to_dataloader(x, y, batch_size=32, shuffle=True):\n",
    "  dataset = CustomDataset(x, y)\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Otimizador**"
   ],
   "metadata": {
    "id": "L5fN4eCvR5a6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nessa parte você só precisa adicionar o otimizador que você vai testar. Deixei a definição do PCGrad de exemplo aqui pra você mas qualquer coisa é só me chamar!"
   ],
   "metadata": {
    "id": "BLAW09vbSB-m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Dict, List, Tuple, Union\n",
    "from abc import abstractmethod\n",
    "\n",
    "class WeightMethod:\n",
    "    def __init__(self, n_tasks: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.n_tasks = n_tasks\n",
    "        self.device = device\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_weighted_loss(\n",
    "        self,\n",
    "        losses: torch.Tensor,\n",
    "        shared_parameters: Union[List[torch.nn.parameter.Parameter], torch.Tensor],\n",
    "        task_specific_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ],\n",
    "        last_shared_parameters: Union[List[torch.nn.parameter.Parameter], torch.Tensor],\n",
    "        representation: Union[torch.nn.parameter.Parameter, torch.Tensor],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def backward(\n",
    "        self,\n",
    "        losses: torch.Tensor,\n",
    "        shared_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        task_specific_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        last_shared_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        representation: Union[List[torch.nn.parameter.Parameter], torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Union[torch.Tensor, None], Union[dict, None]]:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        losses :\n",
    "        shared_parameters :\n",
    "        task_specific_parameters :\n",
    "        last_shared_parameters : parameters of last shared layer/block\n",
    "        representation : shared representation\n",
    "        kwargs :\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Loss, extra outputs\n",
    "        \"\"\"\n",
    "        loss, extra_outputs = self.get_weighted_loss(\n",
    "            losses=losses,\n",
    "            shared_parameters=shared_parameters,\n",
    "            task_specific_parameters=task_specific_parameters,\n",
    "            last_shared_parameters=last_shared_parameters,\n",
    "            representation=representation,\n",
    "            **kwargs,\n",
    "        )\n",
    "        loss.backward()\n",
    "        return loss, extra_outputs\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        losses: torch.Tensor,\n",
    "        shared_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        task_specific_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return self.backward(\n",
    "            losses=losses,\n",
    "            shared_parameters=shared_parameters,\n",
    "            task_specific_parameters=task_specific_parameters,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def parameters(self) -> List[torch.Tensor]:\n",
    "        \"\"\"return learnable parameters\"\"\"\n",
    "        return []"
   ],
   "metadata": {
    "id": "2lWMLJgXSMjo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PCGrad(WeightMethod):\n",
    "    \"\"\"Modification of: https://github.com/WeiChengTseng/Pytorch-PCGrad/blob/master/pcgrad.py\n",
    "\n",
    "    @misc{Pytorch-PCGrad,\n",
    "      author = {Wei-Cheng Tseng},\n",
    "      title = {WeiChengTseng/Pytorch-PCGrad},\n",
    "      url = {https://github.com/WeiChengTseng/Pytorch-PCGrad.git},\n",
    "      year = {2020}\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_tasks: int, device: torch.device, reduction=\"sum\"):\n",
    "        super().__init__(n_tasks, device=device)\n",
    "        assert reduction in [\"mean\", \"sum\"]\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def get_weighted_loss(\n",
    "        self,\n",
    "        losses: torch.Tensor,\n",
    "        shared_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        task_specific_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _set_pc_grads(self, losses, shared_parameters, task_specific_parameters=None):\n",
    "        # shared part\n",
    "        shared_grads = []\n",
    "        for l in losses:\n",
    "            shared_grads.append(\n",
    "                torch.autograd.grad(l, shared_parameters, retain_graph=True, allow_unused=True)\n",
    "            )\n",
    "\n",
    "        if isinstance(shared_parameters, torch.Tensor):\n",
    "            shared_parameters = [shared_parameters]\n",
    "        non_conflict_shared_grads = self._project_conflicting(shared_grads)\n",
    "        for p, g in zip(shared_parameters, non_conflict_shared_grads):\n",
    "            p.grad = g\n",
    "\n",
    "        # task specific part\n",
    "        if task_specific_parameters is not None:\n",
    "            task_specific_grads = torch.autograd.grad(\n",
    "                losses.sum(), task_specific_parameters\n",
    "            )\n",
    "            if isinstance(task_specific_parameters, torch.Tensor):\n",
    "                task_specific_parameters = [task_specific_parameters]\n",
    "            for p, g in zip(task_specific_parameters, task_specific_grads):\n",
    "                p.grad = g\n",
    "\n",
    "    def _project_conflicting(self, grads: List[Tuple[torch.Tensor]]):\n",
    "        pc_grad = copy.deepcopy(grads)\n",
    "        for g_i in pc_grad:\n",
    "            random.shuffle(grads)\n",
    "            for g_j in grads:\n",
    "                g_i_g_j = sum(\n",
    "                    [\n",
    "                        torch.dot(torch.flatten(grad_i), torch.flatten(grad_j))\n",
    "                        for grad_i, grad_j in zip(g_i, g_j)\n",
    "                    ]\n",
    "                )\n",
    "                if g_i_g_j < 0:\n",
    "                    g_j_norm_square = (\n",
    "                        torch.norm(torch.cat([torch.flatten(g) for g in g_j])) ** 2\n",
    "                    )\n",
    "                    for grad_i, grad_j in zip(g_i, g_j):\n",
    "                        grad_i -= g_i_g_j * grad_j / g_j_norm_square\n",
    "\n",
    "        merged_grad = [sum(g) for g in zip(*pc_grad)]\n",
    "        if self.reduction == \"mean\":\n",
    "            merged_grad = [g / self.n_tasks for g in merged_grad]\n",
    "\n",
    "        return merged_grad\n",
    "\n",
    "    def backward(\n",
    "        self,\n",
    "        losses: torch.Tensor,\n",
    "        parameters: Union[List[torch.nn.parameter.Parameter], torch.Tensor] = None,\n",
    "        shared_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        task_specific_parameters: Union[\n",
    "            List[torch.nn.parameter.Parameter], torch.Tensor\n",
    "        ] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._set_pc_grads(losses, shared_parameters, task_specific_parameters)\n",
    "        return None, {}  # NOTE: to align with all other weight methods"
   ],
   "metadata": {
    "id": "tl_RsdRtSPQ6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfQnUuKgIUz5"
   },
   "source": [
    "### **Arquitetura do modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cj_OYEQeRrrC"
   },
   "source": [
    "#### **Arquitetura do modelo NPC**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_length=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: The dimensionality of the embedding space.\n",
    "            max_seq_length: The maximum sequence length to handle.\n",
    "            dropout: Dropout rate applied to positional encodings.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        positional_encoding = torch.zeros(max_seq_length, embed_dim)\n",
    "        positions = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "\n",
    "        positional_encoding[:, 0::2] = torch.sin(positions * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(positions * div_term)\n",
    "\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        Returns:\n",
    "            Tensor with positional encodings added to the input embeddings.\n",
    "        \"\"\"\n",
    "        seq_length = x.size(1)\n",
    "        x = x + self.positional_encoding[:, :seq_length, :]\n",
    "        return self.dropout(x)\n"
   ],
   "metadata": {
    "id": "ogP9pbGxvRTo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8mIQCj-WeYL"
   },
   "outputs": [],
   "source": [
    "class NextPoinet(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes, num_heads, seq_length, num_layers, dropout=0.1):\n",
    "        super(NextPoinet, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_length = seq_length\n",
    "        self.pe = PositionalEncoding(embed_dim, 9)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dim_feedforward=embed_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.linear_layers = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pe(x)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones(seq_length, seq_length) * float('-inf'), diagonal=1).to(x.device)\n",
    "\n",
    "        x = self.transformer_encoder(x, mask=attn_mask)\n",
    "\n",
    "        x = self.linear_layers(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuehE4PiRvfS"
   },
   "source": [
    "#### **Arquitetura do modelo PCat**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CategoryPoinet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CategoryPoinet, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(256, 484)\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        b, seq, dim = x.shape\n",
    "        dim = int(dim ** 0.5)\n",
    "        x = x.view(b, seq, dim, dim)\n",
    "\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = out.view(b, seq, -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "id": "zmwap_SdZk3Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_-c0ilfRzdo"
   },
   "source": [
    "#### **Arquitetura do modelo MTL-POI**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class MTLnet(nn.Module):\n",
    "    def __init__(self, feature_size, shared_layer_size, num_classes, num_heads, num_layers, seq_length, num_shared_layers):\n",
    "        super(MTLnet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_size = feature_size\n",
    "        self.embedding = torch.nn.Embedding(1, feature_size)\n",
    "\n",
    "        shared_linear_layers = [] # lista de layers compartilhadas\n",
    "\n",
    "         # add primeira camada de input (feature_size -> shared_layer_size)\n",
    "        shared_linear_layers.append(nn.Linear(feature_size, shared_layer_size))\n",
    "        shared_linear_layers.append(nn.LeakyReLU())\n",
    "        shared_linear_layers.append(nn.Dropout())\n",
    "\n",
    "        # add layers intermediarias\n",
    "        for _ in range(num_shared_layers - 1):\n",
    "            shared_linear_layers.append(nn.Linear(shared_layer_size, shared_layer_size))\n",
    "            shared_linear_layers.append(nn.LeakyReLU())\n",
    "            shared_linear_layers.append(nn.Dropout())\n",
    "\n",
    "        # cria o sequential igual antes\n",
    "        self.shared_layers = nn.Sequential(*shared_linear_layers)\n",
    "\n",
    "        self.category_poi = CategoryPoinet(num_classes)\n",
    "        self.next_poi = NextPoinet(shared_layer_size, num_classes, num_heads, seq_length, num_layers)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        idxs = x2.sum(-1) == 0\n",
    "\n",
    "        if torch.any(idxs):\n",
    "          replace_tensor = self.embedding(torch.tensor(0, dtype=torch.long).to(device))\n",
    "          x2[idxs] = replace_tensor\n",
    "\n",
    "        shared_output1 = self.shared_layers(x1)\n",
    "        shared_output2 = self.shared_layers(x2)\n",
    "\n",
    "        out1 = self.category_poi(shared_output1)\n",
    "        out2 = self.next_poi(shared_output2)\n",
    "\n",
    "        out1 = out1.view(-1, self.num_classes)\n",
    "\n",
    "        return out1, out2\n",
    "\n",
    "    def forward_nextpoi(self, x):\n",
    "        idxs = x.sum(-1) == 0\n",
    "        x[idxs] = self.embedding(torch.tensor(0,dtype=torch.long).to(device))\n",
    "\n",
    "        shared_output = self.shared_layers(x)\n",
    "\n",
    "        out = self.next_poi(shared_output)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward_categorypoi(self, x):\n",
    "        shared_output = self.shared_layers(x)\n",
    "\n",
    "        out = self.category_poi(shared_output)\n",
    "\n",
    "        out = out.view(-1, self.num_classes)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "id": "tVzyoP6LfPWm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Cálculo do FLOPs do modelo**"
   ],
   "metadata": {
    "id": "pVLZGsv12hU8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = MTLnet(\n",
    "      feature_size=100,\n",
    "      shared_layer_size=256,\n",
    "      num_classes=7,\n",
    "      num_heads=8,\n",
    "      num_layers=4,\n",
    "      seq_length=9,\n",
    "      num_shared_layers=4,\n",
    "    )\n",
    "\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "id": "Pw14Pj7P2puv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d66c479c-835b-4809-ff27-c0261ce675ff"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x_nextpoi_ = x_nextpoi.copy()\n",
    "x_nextpoi_.userid = x_nextpoi_.userid.astype(int)\n",
    "usersids = x_nextpoi_['userid'].unique()\n",
    "\n",
    "train_user_index, test_user_index = train_test_split(usersids, test_size=0.25, random_state=42)\n",
    "\n",
    "train_place_index, test_place_index = train_test_split(x_categorypoi.index, test_size=0.25, random_state=42)\n",
    "\n",
    "train_user_index, val_user_index = train_test_split(train_user_index, test_size=0.25, random_state=42)\n",
    "train_place_index, val_place_index = train_test_split(train_place_index, test_size=0.25, random_state=42)\n",
    "\n",
    "train_index_next = x_nextpoi_[x_nextpoi_['userid'].isin(train_user_index)].index\n",
    "x_nextpoi_ = x_nextpoi_.drop('userid', axis=1)\n",
    "\n",
    "y_nextpoi_ = y_nextpoi.copy()\n",
    "x_nextpoi_, y_nextpoi_ = x_y_to_tensor(x_nextpoi_, y_nextpoi_, 'next')\n",
    "x_train_next = x_nextpoi_[train_index_next]\n",
    "y_train_next = y_nextpoi_[train_index_next]\n",
    "\n",
    "x_categorypoi_ = x_categorypoi.copy()\n",
    "y_categorypoi_ = y_categorypoi.copy()\n",
    "x_categorypoi_, y_categorypoi_ = x_y_to_tensor(x_categorypoi_, y_categorypoi_, 'category')\n",
    "\n",
    "if isinstance(x_categorypoi_, torch.Tensor):\n",
    "    train_place_index, test_place_index = train_test_split(torch.arange(x_categorypoi_.size(0)), test_size=0.25, random_state=42)\n",
    "    train_place_index, val_place_index = train_test_split(train_place_index, test_size=0.25, random_state=42)\n",
    "\n",
    "    x_train_category = x_categorypoi_[train_place_index]\n",
    "    y_train_category = y_categorypoi_[train_place_index]\n",
    "\n",
    "    x_test_category = x_categorypoi_[test_place_index]\n",
    "    y_test_category = y_categorypoi_[test_place_index]\n",
    "\n",
    "    x_val_category = x_categorypoi_[val_place_index]\n",
    "    y_val_category = y_categorypoi_[val_place_index]\n",
    "else:\n",
    "    raise TypeError(\"x_categorypoi should be a tensor\")\n",
    "\n",
    "next_dataloader_train = input_to_dataloader(x_train_next, y_train_next)\n",
    "category_dataloader_train = input_to_dataloader(x_train_category, y_train_category)"
   ],
   "metadata": {
    "id": "ZMw2XJPp68mm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Tamanho de x_categorypoi:\", x_categorypoi.shape[0])\n",
    "print(\"train_place_index:\", train_place_index)\n",
    "print(\"test_place_index:\", test_place_index)\n",
    "print(\"val_place_index:\", val_place_index)\n",
    "print(type(x_categorypoi))"
   ],
   "metadata": {
    "id": "ZKKWaaNB92Ur",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "57f22f11-21ec-49d6-e285-0cad32db2ef4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tamanho de x_categorypoi: 9090\n",
      "train_place_index: tensor([2941, 4620, 7943,  ..., 4455, 7486, 8697])\n",
      "test_place_index: tensor([3290, 2554, 3065,  ..., 4165, 3307, 1302])\n",
      "val_place_index: tensor([8328, 4452, 4330,  ..., 1453, 1439, 5042])\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "first_batch_nextpoi = next(iter(next_dataloader_train))\n",
    "x1 = first_batch_nextpoi['x']\n",
    "\n",
    "first_batch_categorypoi = next(iter(category_dataloader_train))\n",
    "x2 = first_batch_categorypoi['x']\n",
    "\n",
    "x1 = x1.to(device)\n",
    "x2 = x2.to(device)"
   ],
   "metadata": {
    "id": "yxTWex855Rsp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flops, macs, params = calculate_flops(model=model,\n",
    "                                          kwargs={'x1': x2, 'x2': x1},\n",
    "                                          print_results=False)\n",
    "\n",
    "print(\"MTL-Poi FLOPs: %s   MACs: %s   Params: %s\" %(flops, macs, params))\n",
    "\n",
    "flops = {\n",
    "        'flops': flops,\n",
    "        'macs': macs,\n",
    "        'params': params,\n",
    "}"
   ],
   "metadata": {
    "id": "2aPf9yDu5Pay",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9292dcee-7459-4764-9973-65de42067cd9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MTL-Poi FLOPs: 1.38 GFLOPS   MACs: 687.28 MMACs   Params: 2.03 M\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYRHc6pAYPJZ"
   },
   "source": [
    "### **Treinamento e teste do modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phSYs7n_kGfr"
   },
   "source": [
    "#### **Definição da função de validação**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def val_model(model, loss_function, one_hot, dataloader, id_to_name, task_name):\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  running_loss = 0.0\n",
    "  steps = 0\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for data in dataloader:\n",
    "          x, y = data['x'], data['y']\n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "\n",
    "          if task_name == 'next':\n",
    "            out = model.forward_nextpoi(x)\n",
    "\n",
    "            B, S, _ = out.shape\n",
    "            out = out.view(B * S , -1)\n",
    "            y = y.view(B * S , -1)\n",
    "\n",
    "            valid_samples = y < 7\n",
    "            expanded_mask = valid_samples.expand(-1, out.shape[1])\n",
    "\n",
    "            y = y[valid_samples]\n",
    "            out = out[expanded_mask].view(-1, 7)\n",
    "\n",
    "            predicted = torch.argmax(out, dim=-1)\n",
    "            y_true.extend(y.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "            # y_one_hot = one_hot[y]\n",
    "            loss = loss_function(out, y)\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "\n",
    "          else:\n",
    "            out = model.forward_categorypoi(x)\n",
    "\n",
    "            predicted = torch.argmax(out, dim=-1)\n",
    "            y_true.extend(y.tolist())\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "            y_one_hot = one_hot[y]\n",
    "            loss = loss_function(out, y_one_hot)\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "\n",
    "  average_loss = running_loss / steps if steps > 0 else 0.0\n",
    "\n",
    "  print_classification_report_val(y_true, y_pred, id_to_name, task_name)\n",
    "\n",
    "  return y_true, y_pred, average_loss"
   ],
   "metadata": {
    "id": "OIRX_eG2lhu8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Definição da função de teste**"
   ],
   "metadata": {
    "id": "m4tc_46wm6jT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J38Kgh95UQt"
   },
   "outputs": [],
   "source": [
    "def test_model(model, dataloader, id_to_name, task_name):\n",
    "  print(f'\\nTESTE {task_name.upper()}\\n')\n",
    "\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for data in dataloader:\n",
    "          x, y = data['x'], data['y']\n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "\n",
    "          if task_name == 'next':\n",
    "            out = model.forward_nextpoi(x)\n",
    "          else:\n",
    "            out = model.forward_categorypoi(x)\n",
    "\n",
    "          predicted = torch.argmax(out, dim=-1)\n",
    "\n",
    "          valid_samples = y < 7\n",
    "\n",
    "          y = y[valid_samples]\n",
    "          predicted = predicted[valid_samples]\n",
    "\n",
    "          y_true.extend(y.view(-1).tolist())\n",
    "          y_pred.extend(predicted.view(-1).tolist())\n",
    "\n",
    "      print_classification_report_test(y_true, y_pred, id_to_name, task_name)\n",
    "      print()\n",
    "\n",
    "  return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmbkYTTqkMmv"
   },
   "source": [
    "#### **Definição da função de treino**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlQ_L4UHhUY_"
   },
   "outputs": [],
   "source": [
    "def train_model_by_fold(model, optimizer, scheduler, next_dataloader_train, category_dataloader_train, next_dataloader_val,\n",
    "                        category_dataloader_val, loss_function, one_hot, num_epochs, id_to_name, print_interval=1):\n",
    "    mtl_train_losses = []\n",
    "    next_losses = []\n",
    "    category_losses = []\n",
    "    next_val_losses = []\n",
    "    category_val_losses = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pcgrad = PCGrad(n_tasks=2, device=device, reduction=\"sum\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        next_running_loss = 0.0\n",
    "        category_running_loss = 0.0\n",
    "        max_norm = 1.0\n",
    "\n",
    "        y_true_category = []\n",
    "        y_pred_category = []\n",
    "        y_true_next = []\n",
    "        y_pred_next = []\n",
    "\n",
    "        steps = 0\n",
    "        category_iter = cycle(category_dataloader_train)\n",
    "\n",
    "        for data_next in next_dataloader_train:\n",
    "            data_category = next(category_iter)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_next, y_next = data_next['x'], data_next['y']\n",
    "            x_next = x_next.to(device)\n",
    "            y_next = y_next.to(device)\n",
    "\n",
    "            x_category, y_category = data_category['x'], data_category['y']\n",
    "            x_category = x_category.to(device)\n",
    "            y_category = y_category.to(device)\n",
    "\n",
    "            out_category, out_next = model(x_category, x_next)\n",
    "            category_predicted = torch.argmax(out_category, dim=-1)\n",
    "            y_true_category.extend(y_category.tolist())\n",
    "            y_pred_category.extend(category_predicted.tolist())\n",
    "\n",
    "            B, S, _ = out_next.shape\n",
    "            out_next = out_next.view(B * S , -1)\n",
    "            y_next = y_next.view(B * S , -1)\n",
    "            idx_valid = (y_next < num_classes).view(-1)\n",
    "            y_next = y_next[idx_valid].view(-1)\n",
    "            out_next = out_next[idx_valid]\n",
    "            next_predicted = torch.argmax(out_next, dim=-1)\n",
    "            y_true_next.extend(y_next.tolist())\n",
    "            y_pred_next.extend(next_predicted.view(-1).tolist())\n",
    "            out_next = out_next.view(-1, num_classes)\n",
    "\n",
    "            #perdas\n",
    "            loss_next = loss_function(out_next, y_next)\n",
    "            loss_category = loss_function(out_category, y_category.view(-1))\n",
    "            loss = loss_next + loss_category\n",
    "\n",
    "            #aplicar pcgrad\n",
    "            loss_next.backward(retain_graph=True)\n",
    "            loss_category.backward(retain_graph=True)\n",
    "\n",
    "            pcgrad.backward(\n",
    "            losses=torch.stack([loss_next, loss_category]),\n",
    "            shared_parameters=list(model.shared_layers.parameters()),\n",
    "            # shared_parameters=list(model.shared_parameters()),\n",
    "            # task_specific_parameters=list(model.task_specific_parameters())\n",
    "            )\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            steps += 1\n",
    "            running_loss += loss.item()\n",
    "            next_running_loss += loss_next.item()\n",
    "            category_running_loss += loss_category.item()\n",
    "\n",
    "        if steps > 0:\n",
    "            if (epoch + 1) % print_interval == 0 or epoch == 0:\n",
    "                mtl_loss_by_epoch = running_loss / steps\n",
    "                next_loss_by_epoch = next_running_loss / steps\n",
    "                category_loss_by_epoch = category_running_loss / steps\n",
    "\n",
    "                mtl_train_losses.append(mtl_loss_by_epoch)\n",
    "                next_losses.append(next_loss_by_epoch)\n",
    "                category_losses.append(category_loss_by_epoch)\n",
    "                print_train_losses(epoch, num_epochs, mtl_loss_by_epoch, next_loss_by_epoch, category_loss_by_epoch)\n",
    "                print_classification_report_train(y_true_category, y_pred_category, y_true_next, y_pred_next, id_to_name)\n",
    "                print()\n",
    "                print(\"-\" * 110)\n",
    "                print()\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                # validação category\n",
    "                val_y_true_category, val_y_pred_category, category_val_loss = val_model(model, loss_function, one_hot, category_dataloader_val, id_to_name, 'category')\n",
    "                category_val_losses.append(category_val_loss)\n",
    "\n",
    "                # validação next\n",
    "                val_y_true_next, val_y_pred_next, next_val_loss = val_model(model, loss_function, one_hot, next_dataloader_val, id_to_name, 'next')\n",
    "                next_val_losses.append(next_val_loss)\n",
    "                print(\"*\" * 110)\n",
    "\n",
    "                print_val_losses(next_val_loss, category_val_loss)\n",
    "\n",
    "                scheduler.step(mtl_loss_by_epoch)\n",
    "\n",
    "    results = {'mtl_train_losses': mtl_train_losses,\n",
    "               'next_losses': next_losses,\n",
    "               'category_losses': category_losses,\n",
    "               'next_val_losses': next_val_losses,\n",
    "               'category_val_losses': category_val_losses,\n",
    "               'y_true_category': y_true_category,\n",
    "               'y_pred_category': y_pred_category,\n",
    "               'y_true_next': y_true_next,\n",
    "               'y_pred_next': y_pred_next,\n",
    "               'val_y_true_category': val_y_true_category,\n",
    "               'val_y_pred_category': val_y_pred_category,\n",
    "               'val_y_true_next': val_y_true_next,\n",
    "               'val_y_pred_next': val_y_pred_next\n",
    "               }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g7EaBaFknkI"
   },
   "source": [
    "#### **Treinamento do modelo com validação cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_L4d_6XcHUb"
   },
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "num_epochs = 50\n",
    "fold = 1\n",
    "print_interval = 1\n",
    "learning_rate = 0.0001\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "one_hot = torch.eye(num_classes).to(device)\n",
    "\n",
    "try:\n",
    "  x_nextpoi.userid = x_nextpoi.userid.astype(int)\n",
    "  usersids = x_nextpoi['userid'].unique()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "fold_results_test = {}\n",
    "fold_results_train = {}\n",
    "fold_losses_train = {}\n",
    "fold_results_val = {}\n",
    "fold_losses_val = {}\n",
    "\n",
    "for (train_user_index, test_user_index), (train_place_index, test_place_index) in zip(kf.split(usersids), kf.split(x_categorypoi)):\n",
    "    print(\"#\" * 110)\n",
    "    print(f'FOLD {fold}:')\n",
    "\n",
    "    try:\n",
    "      x_nextpoi = _x_nextpoi.copy()\n",
    "      y_nextpoi = _y_nextpoi.copy()\n",
    "      x_categorypoi = _x_categorypoi.copy()\n",
    "      y_categorypoi = _y_categorypoi.copy()\n",
    "    except:\n",
    "      _x_nextpoi = x_nextpoi.copy()\n",
    "      _y_nextpoi = y_nextpoi.copy()\n",
    "      _x_categorypoi = x_categorypoi.copy()\n",
    "      _y_categorypoi = y_categorypoi.copy()\n",
    "\n",
    "    model = MTLnet(\n",
    "      feature_size=100,\n",
    "      shared_layer_size=256,\n",
    "      num_classes=num_classes,\n",
    "      num_heads=8,\n",
    "      num_layers=4,\n",
    "      seq_length=9,\n",
    "      num_shared_layers=4,\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    # val e train\n",
    "    train_user_index, val_user_index = train_test_split(train_user_index, test_size=0.25, random_state=42)\n",
    "    train_place_index, val_place_index = train_test_split(train_place_index, test_size=0.25, random_state=42)\n",
    "\n",
    "    # indexs para o split pro nextpoi\n",
    "    train_index_next = x_nextpoi[x_nextpoi['userid'].isin(usersids[train_user_index])].index\n",
    "    val_index_next = x_nextpoi[x_nextpoi['userid'].isin(usersids[val_user_index])].index\n",
    "    test_index_next = x_nextpoi[x_nextpoi['userid'].isin(usersids[test_user_index])].index\n",
    "    x_nextpoi = x_nextpoi.drop('userid', axis=1)\n",
    "\n",
    "    # train, test e val pro nextpoi\n",
    "    x_nextpoi, y_nextpoi = x_y_to_tensor(x_nextpoi, y_nextpoi, 'next')\n",
    "    x_train_next, x_test_next, x_val_next = x_nextpoi[train_index_next], x_nextpoi[test_index_next], x_nextpoi[val_index_next]\n",
    "    y_train_next, y_test_next, y_val_next = y_nextpoi[train_index_next], y_nextpoi[test_index_next], y_nextpoi[val_index_next]\n",
    "\n",
    "\n",
    "    # train, test e val pro categorypoi\n",
    "    x_categorypoi, y_categorypoi = x_y_to_tensor(x_categorypoi, y_categorypoi, 'category')\n",
    "    x_train_category, x_test_category, x_val_category = x_categorypoi[train_place_index], x_categorypoi[test_place_index], x_categorypoi[val_place_index]\n",
    "    y_train_category, y_test_category, y_val_category = y_categorypoi[train_place_index], y_categorypoi[test_place_index], y_categorypoi[val_place_index]\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_category.cpu().numpy()), y=y_train_category.cpu().numpy())\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # dataloaders nextpoi\n",
    "    next_dataloader_train = input_to_dataloader(x_train_next, y_train_next)\n",
    "    next_dataloader_test = input_to_dataloader(x_test_next, y_test_next)\n",
    "    next_dataloader_val = input_to_dataloader(x_val_next, y_val_next)\n",
    "\n",
    "    # dataloaders categorypoi\n",
    "    category_dataloader_train = input_to_dataloader(x_train_category, y_train_category)\n",
    "    category_dataloader_test = input_to_dataloader(x_test_category, y_test_category)\n",
    "    category_dataloader_val = input_to_dataloader(x_val_category, y_val_category)\n",
    "\n",
    "    fold_results_test[fold] = {\n",
    "        'test_next_true': [],\n",
    "        'test_next_pred': [],\n",
    "        'test_category_true': [],\n",
    "        'test_category_pred': []\n",
    "    }\n",
    "\n",
    "    fold_results_train[fold] = {\n",
    "        'train_next_true': [],\n",
    "        'train_next_pred': [],\n",
    "        'train_category_true': [],\n",
    "        'train_category_pred': [],\n",
    "    }\n",
    "\n",
    "    fold_results_val[fold] = {\n",
    "        'val_next_true': [],\n",
    "        'val_next_pred': [],\n",
    "        'val_category_true': [],\n",
    "        'val_category_pred': [],\n",
    "    }\n",
    "\n",
    "    fold_losses_train[fold] = {\n",
    "        'mtl_train_losses': [],\n",
    "        'next_losses': [],\n",
    "        'category_losses': []\n",
    "    }\n",
    "\n",
    "    fold_losses_val[fold] = {\n",
    "        'next_val_losses': [],\n",
    "        'category_val_losses': [],\n",
    "    }\n",
    "\n",
    "    # train mtl model\n",
    "    results = train_model_by_fold(model, optimizer, scheduler, next_dataloader_train, category_dataloader_train,\n",
    "                                  next_dataloader_val, category_dataloader_val,\n",
    "                                  loss_function, one_hot, num_epochs, id_to_name)\n",
    "\n",
    "    fold_losses_train[fold]['mtl_train_losses'] = results['mtl_train_losses']\n",
    "    fold_losses_train[fold]['next_losses'] = results['next_losses']\n",
    "    fold_losses_train[fold]['category_losses'] = results['category_losses']\n",
    "\n",
    "    fold_losses_val[fold]['next_val_losses'] = results['next_val_losses']\n",
    "    fold_losses_val[fold]['category_val_losses'] = results['category_val_losses']\n",
    "\n",
    "    fold_results_train[fold]['train_next_true'] = results['y_true_next']\n",
    "    fold_results_train[fold]['train_next_pred'] = results['y_pred_next']\n",
    "    fold_results_train[fold]['train_category_true'] = results['y_true_category']\n",
    "    fold_results_train[fold]['train_category_pred'] = results['y_pred_category']\n",
    "\n",
    "    fold_results_val[fold]['val_next_true'] = results['val_y_true_next']\n",
    "    fold_results_val[fold]['val_next_pred'] = results['val_y_pred_next']\n",
    "    fold_results_val[fold]['val_category_true'] = results['val_y_true_category']\n",
    "    fold_results_val[fold]['val_category_pred'] = results['val_y_pred_category']\n",
    "\n",
    "    # test next\n",
    "    y_true_test_next, y_pred_test_next = test_model(model, next_dataloader_test, id_to_name, 'next')\n",
    "    fold_results_test[fold]['test_next_true'] = y_true_test_next\n",
    "    fold_results_test[fold]['test_next_pred'] = y_pred_test_next\n",
    "\n",
    "    # test category\n",
    "    y_true_test_category, y_pred_test_category = test_model(model, category_dataloader_test, id_to_name, 'category')\n",
    "    fold_results_test[fold]['test_category_true'] = y_true_test_category\n",
    "    fold_results_test[fold]['test_category_pred'] = y_pred_test_category\n",
    "\n",
    "    fold += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4JkJj-fPIG27",
    "mDlyO8AOeSOY",
    "RY3646SHTtkl",
    "TkaE_93wjEly",
    "c1FiLVSsj_Y7",
    "Vfx1F_2aq9Rc",
    "77IpbaZOSo-F",
    "o3ffDM-mmbsl",
    "5-QcXfNJ_W4B",
    "oI_sLoXqYJn-",
    "L5fN4eCvR5a6",
    "MfQnUuKgIUz5",
    "pVLZGsv12hU8",
    "phSYs7n_kGfr",
    "m4tc_46wm6jT",
    "DmbkYTTqkMmv"
   ],
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
