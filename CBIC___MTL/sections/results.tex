% Topicos a serem discutidos:
% 1 - Apresentar os dados do gowalla e as metricas de avaliação(De forma breve) //Felipe ✅
% 2 - Descutir sobre  os resultados do modelos MTL comparando com as baseslines 
%  - Discutir sobre o HMRMR como baseline do Category copiar do (Combining Recurrent and Graph Neural Networks to Predict the Next Place’s Category)
%  - Discutir sobre o MHA+PE(next) como baseline para o Next copiar do (Combining Recurrent and Graph Neural Networks to Predict the Next Place’s Category) ✅
% 3 - MTL converge mais rapido

% Section 3: Experimental Setup and Results
\section{Experimental Setup and Results}
\label{sec:experiments}

\subsection{Dataset and Evaluation Metrics}
\label{sec:method:single_task_heads}

To evaluate our model, we use a subset of the public \emph{Gowalla} check-in dataset, originally collected by Cho \textit{et al.}\,\cite{Cho2011}. We focus our analysis on the data from the U.S. state of Florida, which is one of the most active regions in the original dataset. This subset comprises a total of {$N_\text{users}$} users, {$N_\text{poi}$} unique Points-of-Interest (POIs), and {$N_\text{checkins}$} check-ins.

Following previous work, each POI is mapped to one of seven high-level categories: \emph{Community}, \emph{Entertainment}, \emph{Food}, \emph{Nightlife}, \emph{Outdoors}, \emph{Shopping}, and \emph{Travel}. These categories will be used for the POI category prediction task.

\paragraph*{Evaluation protocol}
To ensure a robust evaluation of our models, all experiments were conducted using a 5-fold cross-validation methodology. For every category, we report precision, recall, and $F_{1}$-score, and utilize the macro-average of these metrics to summarize overall performance. Class-wise metrics are crucial because category frequencies are highly skewed in real LBSN data.


\subsection{Discussion and Comparison with Established Models}
In this section, we discuss the performance of our proposed MTL model and its single-task (Single) counterpart. For a comprehensive evaluation, we selected two state-of-the-art approaches as baselines. The Human Mobility Representation Model (HMRM), introduced by Chen et al. (2020) \cite{chen2020modeling}, is designed for POI category classification. HMRM calculates POI embeddings by considering users' temporal visiting patterns and the co-occurrence of locations within an individual's historical trajectory. To quantify the relationship between locations and visiting times, it employs Point-wise Mutual Information (PMI). The model then utilizes matrix factorization techniques to generate latent representations (embeddings) from various inputs, and these embeddings are subsequently used to train a Support Vector Machine (SVM) for predicting POI categories. 

For the task of next POI category prediction, we compare our models against MHA+PE, a solution proposed by Zeng et al. (2019) \cite{zeng2019next}. This model enhances a recurrent neural network with a Multi-Head Attention (MHA) mechanism, originally developed for machine translation by Vaswani et al. (2017) \cite{vaswani2017attention}, and incorporates Positional Encoding (PE). The MHA mechanism allows the model to extract correlations from different parts of a sequence, effectively capturing the relevance of various elements within a user's trajectory, while positional encoding helps to propagate the order of records in a sequence.

\subsubsection{POI Category Classification}
As shown in Table \ref{table:cat}, both our MTL and Single models significantly outperform HMRM \cite{chen2020modeling} across all POI categories in terms of F1-score, precision, and recall. For instance, in the `Shopping' category, our MTL model achieves an F1-score of $62.51 \pm 0.94$ compared to HMRM's $46.69 \pm 0.81$. Similarly, for `Food', the MTL model scores $57.43 \pm 1.46$ against HMRM's $28.44 \pm 0.42$. While both MTL and Single models are competitive, the Single model shows marginally better F1-scores in several categories like `Community' ($53.11 \pm 0.58$) and `Outdoors' ($47.75 \pm 0.89$), whereas the MTL model excels in `Food' and `Shopping'. Overall, our approaches demonstrate a substantial improvement over the HMRM baseline for this task.
\input{tables/category_result}


\subsubsection{Next POI Category Prediction}
The results for next POI category prediction, detailed in Table \ref{table:next}, indicate a competitive performance landscape. The MHA+PE model \cite{zeng2019next} shows strong F1-scores in several categories, notably `Food' ($43.47 \pm 0.50$) and `Shopping' ($43.53 \pm 1.66$), outperforming our MTL and Single models in these instances. However, our MTL model achieves the best F1-score in `Travel' ($64.61 \pm 1.11$) and `Nightlife' ($22.07 \pm 0.52$), significantly surpassing MHA+PE in these categories. The Single model also shows competitive results, leading in `Entertainment' ($26.06 \pm 1.01$) and `Outdoors' ($22.45 \pm 0.81$). This suggests that while MHA+PE is highly effective for certain POI categories, our MTL and Single models offer superior or comparable performance in others, particularly where different sequential patterns might be leveraged. Also, it is important to notice that since we have an unbalanced result for the MTL and single, this could lead to the worse of other results.
\input{tables/next_result}

Overall, while our proposed models show strong performance in POI category classification against HMRM \cite{chen2020modeling}, the next POI category prediction task presents a more competitive landscape when compared against a sophisticated sequential model like MHA+PE \cite{zeng2019next}. Comparing the MTL and Single models directly reveals that the multi-task learning approach did not yield the anticipated substantial improvements over the single-task baseline across both tasks. Crucially, many of these observed differences in F1-scores are minor and often fall within the reported standard deviations, suggesting that, statistically, the performance of the MTL and Single models is largely comparable, without a clear, consistent, and significant advantage for the multi-task learning setup in these experiments.

\subsection{Convergence Comparison}
To further evaluate the practical implications of the MTL approach compared to single-task models, we conducted a convergence experiment. We measured the wall time, number of epochs, and Mega Floating Point Operations (MFLOPs) required for the MTL model and the individual single-task models (SingleClass and SinglePred) to reach predefined target average F1-scores. The target F1-score for the \textit{Category} prediction task was set to 47, and for the \textit{Next} POI category classification task, it was set to 32.2. These specific F1 values were chosen because they are close to the best-achieved results for each respective task, thereby representing a comparable and significant level of predictive performance for the models to attain. To ensure robust measurements for this experiment, all models were evaluated through a 5-fold cross-validation process. The specific metrics are detailed in Table \ref{table:convergence_comparison}.

\input{tables/converge_result}

The results from this experiment were contrary to the potential expectation that an MTL framework might offer training efficiencies. Our findings indicated that the MTL model took substantially longer to converge to the target F1-scores. Specifically, the MTL approach required almost four times more wall time compared to the cumulative time of the individual single-task models. Furthermore, the computational cost, when considering MFLOPs, was roughly double for the MTL setup. This suggests that while attempting to learn multiple tasks simultaneously, the MTL model, in this configuration, incurred a significantly higher overhead in both time and computational resources. Given these observations, for deployment scenarios where convergence speed and computational efficiency are critical factors, employing separate, optimized single-task models would likely be a more convenient and resource-efficient strategy.\footnote{Experiments conducted on Apple M2 Pro (10-core CPU, 16-core GPU) with 32GB unified memory running macOS 15.5. Key software versions: Python 3.9.6, PyTorch 2.6.0.dev20241014+cpu, NumPy 1.26.4, scikit-learn 1.5.2, CVXPY 1.5.2. Training utilized PyTorch MPS backend for Apple Silicon acceleration.}


%