{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "RQUrs908JwZP",
    "Ogy1r9kkI39n",
    "6bU0q-AQItx4",
    "0BqM9bVMPo3P",
    "a-ypHX4BPrIC",
    "8ncjzcqcPs5F",
    "6y1Ex-DMPvSa",
    "adEWOCkqORx0",
    "UgeKISxYMs-_"
   ],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### **Bibliotecas**"
   ],
   "metadata": {
    "id": "RQUrs908JwZP"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eg8yp0O94_h_",
    "ExecuteTime": {
     "end_time": "2025-03-08T16:45:17.392431Z",
     "start_time": "2025-03-08T16:45:17.221456Z"
    }
   },
   "source": [
    "from os import mkdir\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.linalg import inv as inverse\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T16:46:32.444644Z",
     "start_time": "2025-03-08T16:46:32.439910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IO_ROOT = '../data'\n",
    "IO_CHECKINS = os.path.join(IO_ROOT, 'checkins')\n",
    "if not os.path.exists(IO_CHECKINS):\n",
    "    print('checkins folder does not exist')\n",
    "\n",
    "OUTPUT_ROOT = os.path.join(IO_ROOT, 'output')\n",
    "if not os.path.exists(OUTPUT_ROOT):\n",
    "    mkdir(OUTPUT_ROOT)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Optimizer**"
   ],
   "metadata": {
    "id": "Ogy1r9kkI39n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Optimizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._user_location_frequency = np.array([])\n",
    "        self._user_time_frequency = np.array([])\n",
    "        self._location_co_ocurrency = np.array([])\n",
    "        self._location_time = np.array([])\n",
    "        self._weight = 0.001\n",
    "        self.activity_location = np.array([])\n",
    "        self.activity_time = np.array([])\n",
    "        self.user_activity = np.array([])\n",
    "        self.activity_embedding = np.array([])\n",
    "        self.target_location_embedding = np.array([])\n",
    "        self.context_location_embedding = np.array([])\n",
    "        self.time_slot_embedding = np.array([])\n",
    "\n",
    "    def _create_user_location_frequency_matrix(self, users_checkins):\n",
    "        placeids = users_checkins[\"placeid\"].tolist()\n",
    "        userids = users_checkins[\"userid\"]\n",
    "        total_users = len(users_checkins[\"userid\"].unique())\n",
    "        total_places = len(users_checkins[\"placeid\"].unique())\n",
    "        print(f'total_places: {total_places}, total_users: {total_users}\\n')\n",
    "\n",
    "        self._user_location_frequency = sparse.lil_matrix((total_users, total_places))\n",
    "\n",
    "        for i in range(len(placeids)):\n",
    "            self._user_location_frequency[userids[i], placeids[i]] += 1\n",
    "\n",
    "    def _create_user_time_frequency_matrix(self, users_checkins: pd.DataFrame):\n",
    "        users_checkins_sorted = users_checkins.sort_values(by=[\"datetime\"])\n",
    "\n",
    "        users_ids = users_checkins_sorted[\"userid\"]\n",
    "        datetimes = pd.to_datetime(users_checkins[\"datetime\"])\n",
    "        total_users = len(users_checkins[\"userid\"].unique())\n",
    "\n",
    "        self._user_time_frequency = np.zeros((total_users, 48))\n",
    "\n",
    "        for i, j in zip(users_ids, datetimes):\n",
    "            if j.weekday() >= 5:\n",
    "                self._user_time_frequency[i][j.hour + 24] += 1\n",
    "            else:\n",
    "                self._user_time_frequency[i][j.hour] += 1\n",
    "\n",
    "    def _create_location_coocurrency_matrix(self, users_checkins):\n",
    "        try:\n",
    "            users_checkins_sorted = users_checkins.sort_values(by=[\"datetime\"])\n",
    "            locations = users_checkins_sorted[\"placeid\"].tolist()\n",
    "            number_of_locations = len(users_checkins[\"placeid\"].unique())\n",
    "\n",
    "            self._location_co_ocurrency = sparse.lil_matrix(\n",
    "                (number_of_locations, number_of_locations)\n",
    "            )  ##location co occurency represents memory for save memory\n",
    "\n",
    "            for i in range(len(locations)):\n",
    "                for j in range(1, 6):\n",
    "                    if (i - j) < 0:\n",
    "                        break\n",
    "                    self._location_co_ocurrency[locations[i], locations[i - j]] += 1\n",
    "                for j in range(1, 6):\n",
    "                    if (i + j) > len(locations) - 1:\n",
    "                        break\n",
    "                    self._location_co_ocurrency[locations[i], locations[j + i]] += 1\n",
    "            sum_of_dl = np.sum(self._location_co_ocurrency)\n",
    "            l_occurrency = np.sum(self._location_co_ocurrency, axis=1).reshape(-1, 1)\n",
    "            c_occurrency = np.sum(self._location_co_ocurrency, axis=0).reshape(1, -1)\n",
    "\n",
    "            for i in range(number_of_locations):\n",
    "                line = self._location_co_ocurrency[i].toarray()\n",
    "                ##PMI em subdivisoes da matriz esparsa\n",
    "                self._location_co_ocurrency[i] = np.maximum(\n",
    "                    np.log2(\n",
    "                        np.maximum(line * sum_of_dl, 1)\n",
    "                        / (l_occurrency[i] * c_occurrency)\n",
    "                    ),\n",
    "                    0,\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def _create_location_time_matrix(self, users_checinks):\n",
    "        locations = users_checinks[\"placeid\"].tolist()\n",
    "        datetimes = users_checinks[\"datetime\"].tolist()\n",
    "        total_locations = len(users_checinks[\"placeid\"].unique())\n",
    "        Dt = np.zeros((total_locations, 48))\n",
    "\n",
    "        for i in range(len(locations)):\n",
    "            if datetimes[i].weekday() >= 5:\n",
    "                Dt[locations[i]][datetimes[i].hour + 24] += 1\n",
    "            else:\n",
    "                Dt[locations[i]][datetimes[i].hour] += 1\n",
    "\n",
    "        sum_of_dt = np.sum(Dt)\n",
    "        l_occurrency = np.sum(Dt, axis=1).reshape(-1, 1)\n",
    "        c_occurrency = np.sum(Dt, axis=0).reshape(1, -1)\n",
    "\n",
    "        mult = l_occurrency * c_occurrency\n",
    "        mult[mult == 0] = -1\n",
    "\n",
    "        tmp = np.maximum(Dt * sum_of_dt, 1) / mult\n",
    "        tmp[tmp < 0] = 0\n",
    "        self._location_time = np.maximum(np.log2(tmp), 0)\n",
    "\n",
    "    def _objective_function(self, l2_weight):\n",
    "        def first_component(l2_weight):\n",
    "            first_equation = l2_weight * norm(\n",
    "                (\n",
    "                        self._user_location_frequency\n",
    "                        - np.dot(self.user_activity, self.activity_location.T)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            second_equation = (1 - l2_weight) * norm(\n",
    "                (\n",
    "                        self._user_time_frequency\n",
    "                        - np.dot(self.user_activity, self.activity_time.T)\n",
    "                )\n",
    "            )\n",
    "            return first_equation + second_equation\n",
    "\n",
    "        def second_component(l2_weight):\n",
    "            first_equation = l2_weight * norm(\n",
    "                (\n",
    "                        self._location_co_ocurrency\n",
    "                        - np.dot(\n",
    "                    self.target_location_embedding,\n",
    "                    self.context_location_embedding.T,\n",
    "                )\n",
    "                )\n",
    "            )\n",
    "            second_equation = (1 - l2_weight) * norm(\n",
    "                (\n",
    "                        self._location_time\n",
    "                        - np.dot(self.target_location_embedding, self.time_slot_embedding.T)\n",
    "                )\n",
    "            )\n",
    "            return first_equation + second_equation\n",
    "\n",
    "        def third_component(l2_weight):\n",
    "            first_equation = l2_weight * norm(\n",
    "                (\n",
    "                        self.activity_location\n",
    "                        - np.dot(self.context_location_embedding, self.activity_embedding.T)\n",
    "                )\n",
    "            )\n",
    "            second_equation = (1 - l2_weight) * norm(\n",
    "                (\n",
    "                        self.activity_time\n",
    "                        - np.dot(self.time_slot_embedding, self.activity_embedding.T)\n",
    "                )\n",
    "            )\n",
    "            return first_equation + second_equation\n",
    "\n",
    "        activity_modeling_component = first_component(l2_weight)\n",
    "        trajectory_embedding_component = second_component(l2_weight)\n",
    "        collaborative_learning_component = third_component(l2_weight)\n",
    "\n",
    "        objective_function = (\n",
    "                activity_modeling_component\n",
    "                + trajectory_embedding_component\n",
    "                + collaborative_learning_component\n",
    "        )\n",
    "        objective_function += self._weight * norm(self.user_activity)\n",
    "        objective_function += self._weight * norm(self.activity_time)\n",
    "        objective_function += self._weight * norm(self.activity_embedding)\n",
    "        objective_function += self._weight * norm(self.activity_location)\n",
    "        objective_function += self._weight * norm(self.context_location_embedding)\n",
    "        objective_function += self._weight * norm(self.target_location_embedding)\n",
    "        objective_function += self._weight * norm(self.time_slot_embedding)\n",
    "\n",
    "        return objective_function\n",
    "\n",
    "    def _initialize_parameters(self, checkins, K, M):\n",
    "        total_locations = len(checkins[\"placeid\"].unique())\n",
    "        total_users = len(checkins[\"userid\"].unique())\n",
    "        time_slot = 48\n",
    "\n",
    "        # print(\"\\nDurante a construção:\")\n",
    "        self.activity_location = np.random.normal(size=(total_locations, K))\n",
    "        # print(\"activity location:\", self.activity_location.shape)\n",
    "\n",
    "        self.activity_time = np.random.normal(size=(time_slot, K))\n",
    "        # print(\"activity time:\", self.activity_time.shape)\n",
    "\n",
    "        self.user_activity = np.random.normal(size=(total_users, K))\n",
    "        # print(\"user activity:\", self.user_activity.shape)\n",
    "\n",
    "        self.activity_embedding = np.random.normal(size=(K, M))\n",
    "        # print(\"activity embedding:\", self.activity_embedding.shape)\n",
    "\n",
    "        self.target_location_embedding = np.random.normal(size=(total_locations, M))\n",
    "        # print(\"target location embedding:\", self.target_location_embedding.shape)\n",
    "\n",
    "        self.context_location_embedding = np.random.normal(size=(total_locations, M))\n",
    "        # print(\"context location embedding:\", self.context_location_embedding.shape)\n",
    "\n",
    "        self.time_slot_embedding = np.random.normal(size=(time_slot, M))\n",
    "        # print(\"time slot embedding:\", self.time_slot_embedding.shape)\n",
    "\n",
    "    def user_activity_embedding_function(self, K, l2_weight):\n",
    "        first_equation = (\n",
    "                                 l2_weight * (self._user_location_frequency * self.activity_location)\n",
    "                         ) + ((1 - l2_weight) * np.dot(self._user_time_frequency, self.activity_time))\n",
    "        second_equation = (\n",
    "                                  l2_weight * np.dot(self.activity_location.T, self.activity_location)\n",
    "                          ) + (\n",
    "                                  (1 - l2_weight) * np.dot(self.activity_time.T, self.activity_time)\n",
    "                                  + (l2_weight * np.identity(K))\n",
    "                          )\n",
    "        return np.dot(first_equation, inverse(second_equation))\n",
    "\n",
    "    def acticity_location_embedding_function(self, K, l2_weight):\n",
    "        first_equation = l2_weight * (\n",
    "                (self._user_location_frequency.T * self.user_activity)\n",
    "                + np.dot(self.context_location_embedding, self.activity_embedding.T)\n",
    "        )\n",
    "        second_equation = (\n",
    "                                  l2_weight * np.dot(self.user_activity.T, self.user_activity)\n",
    "                          ) + ((self._weight + l2_weight) * np.identity(K))\n",
    "        return np.dot(first_equation, inverse(second_equation))\n",
    "\n",
    "    def activity_time_embedding_function(self, K, l2_weight):\n",
    "        first_equation = (1 - l2_weight) * (\n",
    "                np.dot(self._user_time_frequency.T, self.user_activity)\n",
    "                + np.dot(self.time_slot_embedding, self.activity_embedding.T)\n",
    "        )\n",
    "        second_equation = (1 - l2_weight) * (\n",
    "                np.dot(self.user_activity.T, self.user_activity)\n",
    "                + (1 - self._weight + l2_weight) * np.identity(K)\n",
    "        )\n",
    "        return np.dot(first_equation, inverse(second_equation))\n",
    "\n",
    "    def activity_embedding_function(self, M, l2_weight):\n",
    "        first_equation = (\n",
    "                                 l2_weight\n",
    "                                 * np.dot(self.activity_location.T, self.context_location_embedding)\n",
    "                         ) + ((1 - l2_weight) * np.dot(self.activity_time.T, self.time_slot_embedding))\n",
    "        second_equation = (\n",
    "                (\n",
    "                        l2_weight\n",
    "                        * np.dot(\n",
    "                    self.context_location_embedding.T, self.context_location_embedding\n",
    "                )\n",
    "                )\n",
    "                + (\n",
    "                        (1 - l2_weight)\n",
    "                        * np.dot(self.time_slot_embedding.T, self.time_slot_embedding)\n",
    "                )\n",
    "                + (self._weight * np.identity(M))\n",
    "        )\n",
    "        return np.dot(first_equation, inverse(second_equation))\n",
    "\n",
    "    def target_location_embedding_function(self, M, l2_weight):\n",
    "        first_equation = (\n",
    "                                 l2_weight * self._location_co_ocurrency * self.context_location_embedding\n",
    "                         ) + ((1 - l2_weight) * np.dot(self._location_time, self.time_slot_embedding))\n",
    "\n",
    "        second_equation = (\n",
    "                (\n",
    "                        l2_weight\n",
    "                        * np.dot(\n",
    "                    self.context_location_embedding.T, self.context_location_embedding\n",
    "                )\n",
    "                )\n",
    "                + (\n",
    "                        (1 - l2_weight)\n",
    "                        * np.dot(self.time_slot_embedding.T, self.time_slot_embedding)\n",
    "                )\n",
    "                + (self._weight * np.identity(M))\n",
    "        )\n",
    "\n",
    "        return np.dot(first_equation, inverse(second_equation))\n",
    "\n",
    "    def context_location_embedding_function(self, M, l2_weight):\n",
    "        first_equation = l2_weight * (\n",
    "                self._location_co_ocurrency.T * self.target_location_embedding\n",
    "                + np.dot(self.activity_location, self.activity_embedding)\n",
    "        )\n",
    "        second_equation = (\n",
    "                                  l2_weight\n",
    "                                  * (\n",
    "                                          np.dot(self.target_location_embedding.T, self.target_location_embedding)\n",
    "                                          + np.dot(self.activity_embedding.T, self.activity_embedding)\n",
    "                                  )\n",
    "                          ) + (self._weight * np.identity(M))\n",
    "        return np.dot(first_equation, inverse(second_equation))\n",
    "\n",
    "    def time_slot_embedding_function(self, M, l2_weight):\n",
    "        first_equation = (1 - l2_weight) * (\n",
    "                np.dot(self._location_time.T, self.target_location_embedding)\n",
    "                + np.dot(self.activity_time, self.activity_embedding)\n",
    "        )\n",
    "        second_equation = (\n",
    "                                  (1 - l2_weight)\n",
    "                                  * (\n",
    "                                          np.dot(self.target_location_embedding.T, self.target_location_embedding)\n",
    "                                          + np.dot(self.activity_embedding.T, self.activity_embedding)\n",
    "                                  )\n",
    "                          ) + (self._weight * np.identity(M))\n",
    "        return np.dot(first_equation, inverse(second_equation))\n",
    "\n",
    "    def _optimize_parameters(self, K, M, l2_weight):\n",
    "        self.user_activity = self.user_activity_embedding_function(K, l2_weight)\n",
    "        self.user_activity[self.user_activity < 0] = 0\n",
    "\n",
    "        self.activity_location = self.acticity_location_embedding_function(K, l2_weight)\n",
    "        self.activity_location[self.activity_location < 0] = 0\n",
    "\n",
    "        self.activity_time = self.activity_time_embedding_function(K, l2_weight)\n",
    "        self.activity_time[self.activity_time < 0] = 0\n",
    "\n",
    "        self.activity_embedding = self.activity_embedding_function(M, l2_weight)\n",
    "        self.target_location_embedding = self.target_location_embedding_function(\n",
    "            M, l2_weight\n",
    "        )\n",
    "        self.context_location_embedding = self.context_location_embedding_function(\n",
    "            M, l2_weight\n",
    "        )\n",
    "        self.time_slot_embedding = self.time_slot_embedding_function(M, l2_weight)\n",
    "\n",
    "    def start(self, checkins, l2_weight=0.1, K=10, M=100):\n",
    "        print(f'\\nInicando o HMRM...')\n",
    "        checkins[\"datetime\"] = pd.to_datetime(checkins[\"datetime\"])\n",
    "\n",
    "        self._create_user_location_frequency_matrix(checkins)\n",
    "        self._create_location_coocurrency_matrix(checkins)\n",
    "        self._create_user_time_frequency_matrix(checkins)\n",
    "        self._create_location_time_matrix(checkins)\n",
    "\n",
    "        print(f'\\nMatrizes criadas...')\n",
    "\n",
    "        self._initialize_parameters(checkins, K, M)\n",
    "\n",
    "        value = 100000\n",
    "\n",
    "        print(\"\\nOtimizando os parâmetros\")\n",
    "        for i in range(10):\n",
    "            print(i)\n",
    "            self._optimize_parameters(K, M, l2_weight)\n",
    "            objective_func = self._objective_function(l2_weight)\n",
    "\n",
    "            # print(\"user activity:\", self.user_activity) # theta\n",
    "            # print(\"activity location:\", self.activity_location) # Al\n",
    "            # print(\"activity time:\", self.activity_time) # At\n",
    "            # print(\"activity embedding:\", self.activity_embedding) # Ea\n",
    "            # print(\"target location embedding:\", self.target_location_embedding) # El\n",
    "            # print(\"context location embedding:\", self.context_location_embedding) # Ec\n",
    "            # print(\"time slot embedding:\", self.time_slot_embedding) # Et\n",
    "\n",
    "            if (value - objective_func) <= 0.1:\n",
    "                break\n",
    "            value = objective_func"
   ],
   "metadata": {
    "id": "C4bjHwZ5I_Wb",
    "ExecuteTime": {
     "end_time": "2025-03-08T16:46:33.227826Z",
     "start_time": "2025-03-08T16:46:33.204649Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **HMRM Baseline**"
   ],
   "metadata": {
    "id": "6bU0q-AQItx4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class HmrmBaseline:\n",
    "    def __init__(self, file=None, weight=0.5, K=7, embedding_size=50):\n",
    "        self.optimizer = Optimizer()\n",
    "        self.input_file = file\n",
    "        self.weight = weight\n",
    "        self.K = K\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "    def start(self):\n",
    "        users_checkin_filename = self.input_file\n",
    "        users_checkin = pd.read_csv(users_checkin_filename, index_col=False).dropna(\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        usersid = users_checkin.userid\n",
    "\n",
    "        placeid_mapping = dict(zip(range(users_checkin['placeid'].unique().size), users_checkin['placeid'].unique()))\n",
    "\n",
    "        users_checkin.userid = pd.factorize(users_checkin.userid)[0].astype(int)\n",
    "        users_checkin.placeid = pd.factorize(users_checkin.placeid)[0].astype(int)\n",
    "\n",
    "        self.optimizer.start(users_checkin, self.weight, self.K, self.embedding_size)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            data=np.concatenate(\n",
    "                (\n",
    "                    self.optimizer.context_location_embedding,\n",
    "                    self.optimizer.target_location_embedding,\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            values = []\n",
    "            for i in range(df.shape[0]):\n",
    "                category = users_checkin[users_checkin[\"placeid\"] == i][\n",
    "                    \"category\"\n",
    "                ].unique()[0]\n",
    "\n",
    "                values.append(category)\n",
    "\n",
    "            df[\"category\"] = values\n",
    "            df['placeid'] = list(map(lambda x: placeid_mapping[x], range(df.shape[0])))\n",
    "\n",
    "        except Exception as e:\n",
    "            print('vim pro except')\n",
    "            print(f'erro: {e}')\n",
    "            pass\n",
    "\n",
    "        return df"
   ],
   "metadata": {
    "id": "QdFd8Z3XIwKS",
    "ExecuteTime": {
     "end_time": "2025-03-08T16:46:34.016880Z",
     "start_time": "2025-03-08T16:46:34.007336Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Gerando os embeddings gerais com hmrm**"
   ],
   "metadata": {
    "id": "cK2Yf48gJshI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def etl_checkins(df: pd.DataFrame):\n",
    "    \"\"\"Clean and filter check-ins data, keeping only users with 40+ check-ins.\"\"\"\n",
    "    print(f'Original checkins: {df.shape}')\n",
    "\n",
    "    # Standardize datetime column name\n",
    "    if 'local_datetime' in df.columns:\n",
    "        df.rename(columns={'local_datetime': 'datetime'}, inplace=True)\n",
    "        print(f\"Renamed 'local_datetime' to 'datetime'\")\n",
    "\n",
    "    # Filter users with at least 40 check-ins\n",
    "    checkins_per_user = df['userid'].value_counts()\n",
    "    selected_users = checkins_per_user[checkins_per_user >= 40]\n",
    "    users_ids = selected_users.index.unique().tolist()\n",
    "\n",
    "    print(f'Number of qualified users: {len(users_ids)}')\n",
    "\n",
    "    filtered_checkins = df[df['userid'].isin(users_ids)]\n",
    "    print(f'Filtered checkins shape: {filtered_checkins.shape}')\n",
    "\n",
    "    return filtered_checkins\n",
    "\n",
    "\n",
    "def create_embeddings(input_file, weight=0.1, K=7, embedding_size=50):\n",
    "    \"\"\"Generate embeddings using HMRM with specified parameters.\"\"\"\n",
    "    print(f'Creating embeddings with weight={weight}, K={K}, embedding_size={embedding_size}')\n",
    "    hmrm = HmrmBaseline(input_file, weight, K, embedding_size)\n",
    "    return hmrm.start()\n",
    "\n",
    "\n",
    "def embeddings_job(state_name, path, **kwargs):\n",
    "    \"\"\"Process checkins for a state and generate embeddings.\"\"\"\n",
    "    print(f'\\nProcessing {state_name.capitalize()} check-ins...')\n",
    "\n",
    "    try:\n",
    "        # Create state output directory if it doesn't exist\n",
    "        state_dir = f'{OUTPUT_ROOT}/{state_name}'\n",
    "        if not os.path.exists(state_dir):\n",
    "            os.makedirs(state_dir)\n",
    "            print(f'Created directory: {state_dir}')\n",
    "\n",
    "        # Clean and preprocess the checkins data\n",
    "        df = pd.read_csv(path, index_col=False)\n",
    "        df = etl_checkins(df)\n",
    "\n",
    "        # Save the filtered checkins\n",
    "        etl_path = f'{state_dir}/{state_name}-filtrado.csv'\n",
    "        df.to_csv(etl_path, index=False)\n",
    "        print(f'Filtered check-ins saved to {etl_path}')\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = create_embeddings(etl_path,\n",
    "                                       kwargs.get('weight', 0.1),\n",
    "                                       kwargs.get('K', 7),\n",
    "                                       kwargs.get('embedding_size', 50)\n",
    "                                       )\n",
    "\n",
    "        # Save the embeddings\n",
    "        embb_path = f'{state_dir}/{state_name}-embeddings.csv'\n",
    "        embeddings.to_csv(embb_path, index=False)\n",
    "\n",
    "        print(f'Shape: {embeddings.shape}')\n",
    "        print(f'Embeddings for {state_name.capitalize()} generated successfully')\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {state_name}: {str(e)}')\n",
    "        raise"
   ],
   "metadata": {
    "id": "T0exkMtlnzZc",
    "ExecuteTime": {
     "end_time": "2025-03-08T16:55:18.147682Z",
     "start_time": "2025-03-08T16:55:18.142470Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "path_alabama = os.path.join(IO_CHECKINS, 'checkins_Alabama.csv')\n",
    "path_arizona = os.path.join(IO_CHECKINS, 'checkins_Arizona.csv')\n",
    "path_virginia = os.path.join(IO_CHECKINS, 'checkins_Virginia.csv')\n",
    "path_chicago = os.path.join(IO_CHECKINS, 'checkins_Illinois.csv')\n",
    "path_florida = os.path.join(IO_CHECKINS, 'checkins_florida.csv')\n",
    "path_georgia = os.path.join(IO_CHECKINS, 'checkins_Georgia.csv')\n",
    "path_nova_york = os.path.join(IO_CHECKINS, 'checkins_New York.csv')\n",
    "path_texas = os.path.join(IO_CHECKINS, 'checkins_Texas.csv')"
   ],
   "metadata": {
    "id": "jCP5AQMWq4Y3",
    "ExecuteTime": {
     "end_time": "2025-03-08T17:12:10.690273Z",
     "start_time": "2025-03-08T17:12:10.687248Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### alabama"
   ],
   "metadata": {
    "id": "0BqM9bVMPo3P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# alabama\n",
    "_ = embeddings_job('alabama', path_alabama, weight=0.1, K=7, embedding_size=50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "lE45u3OPNPrJ",
    "outputId": "eefc5b94-4407-4775-e438-c32ac60ae0c2",
    "ExecuteTime": {
     "end_time": "2025-03-08T16:55:36.717939Z",
     "start_time": "2025-03-08T16:55:21.690372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Alabama check-ins...\n",
      "Original checkins: (93402, 8)\n",
      "Renamed 'local_datetime' to 'datetime'\n",
      "Number of qualified users: 418\n",
      "Filtered checkins shape: (76041, 8)\n",
      "Filtered check-ins saved to ../data/output/alabama/alabama-filtrado.csv\n",
      "Creating embeddings with weight=0.1, K=7, embedding_size=50\n",
      "\n",
      "Inicando o HMRM...\n",
      "total_places: 9090, total_users: 418\n",
      "\n",
      "\n",
      "Matrizes criadas...\n",
      "\n",
      "Otimizando os parâmetros\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Shape: (9090, 102)\n",
      "Embeddings for Alabama generated successfully\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### arizona"
   ],
   "metadata": {
    "id": "a-ypHX4BPrIC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# arizona\n",
    "_ = embeddings_job('arizona', path_arizona, weight=0.1, K=7, embedding_size=50)"
   ],
   "metadata": {
    "id": "qRPl06hnI2Lt",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "outputId": "9bbc6afd-ad29-4b0d-a436-c5e1c1739291",
    "ExecuteTime": {
     "end_time": "2025-03-08T16:56:19.868141Z",
     "start_time": "2025-03-08T16:55:36.722974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Arizona check-ins...\n",
      "Original checkins: (188860, 8)\n",
      "Renamed 'local_datetime' to 'datetime'\n",
      "Number of qualified users: 756\n",
      "Filtered checkins shape: (152210, 8)\n",
      "Filtered check-ins saved to ../data/output/arizona/arizona-filtrado.csv\n",
      "Creating embeddings with weight=0.1, K=7, embedding_size=50\n",
      "\n",
      "Inicando o HMRM...\n",
      "total_places: 16357, total_users: 756\n",
      "\n",
      "\n",
      "Matrizes criadas...\n",
      "\n",
      "Otimizando os parâmetros\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Shape: (16357, 102)\n",
      "Embeddings for Arizona generated successfully\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### virginia"
   ],
   "metadata": {
    "id": "8ncjzcqcPs5F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# viriginia\n",
    "_ = embeddings_job('virginia', path_virginia, weight=0.1, K=7, embedding_size=50)"
   ],
   "metadata": {
    "id": "c3NvLTU5Xga9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "outputId": "e316ed9c-da53-4fa2-e883-436bf379265e",
    "ExecuteTime": {
     "end_time": "2025-03-08T16:57:32.470388Z",
     "start_time": "2025-03-08T16:56:19.873495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Virginia check-ins...\n",
      "Created directory: ../data/output/virginia\n",
      "Original checkins: (247600, 8)\n",
      "Renamed 'local_datetime' to 'datetime'\n",
      "Number of qualified users: 1059\n",
      "Filtered checkins shape: (195378, 8)\n",
      "Filtered check-ins saved to ../data/output/virginia/virginia-filtrado.csv\n",
      "Creating embeddings with weight=0.1, K=7, embedding_size=50\n",
      "\n",
      "Inicando o HMRM...\n",
      "total_places: 20947, total_users: 1059\n",
      "\n",
      "\n",
      "Matrizes criadas...\n",
      "\n",
      "Otimizando os parâmetros\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Shape: (20947, 102)\n",
      "Embeddings for Virginia generated successfully\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### chicago"
   ],
   "metadata": {
    "id": "6y1Ex-DMPvSa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# chicago\n",
    "_ = embeddings_job('chicago', path_chicago, weight=0.1, K=7, embedding_size=50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYImej4IsP7v",
    "outputId": "ec12d91b-bf52-4e2d-e6ea-d152d21877fd",
    "ExecuteTime": {
     "end_time": "2025-03-08T17:02:42.995742Z",
     "start_time": "2025-03-08T17:00:08.146258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Chicago check-ins...\n",
      "Original checkins: (449674, 8)\n",
      "Renamed 'local_datetime' to 'datetime'\n",
      "Number of qualified users: 1563\n",
      "Filtered checkins shape: (374135, 8)\n",
      "Filtered check-ins saved to ../data/output/chicago/chicago-filtrado.csv\n",
      "Creating embeddings with weight=0.1, K=7, embedding_size=50\n",
      "\n",
      "Inicando o HMRM...\n",
      "total_places: 31676, total_users: 1563\n",
      "\n",
      "\n",
      "Matrizes criadas...\n",
      "\n",
      "Otimizando os parâmetros\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Shape: (31676, 102)\n",
      "Embeddings for Chicago generated successfully\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### georgia"
   ],
   "metadata": {
    "id": "adEWOCkqORx0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# georgia\n",
    "_ = embeddings_job('georgia', path_georgia, weight=0.1, K=7, embedding_size=50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFiTrL8qOWwr",
    "outputId": "9a0538a4-0d68-439d-fe00-6b4ffd38970e",
    "ExecuteTime": {
     "end_time": "2025-03-08T17:04:04.016400Z",
     "start_time": "2025-03-08T17:02:43.038467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Georgia check-ins...\n",
      "Created directory: ../data/output/georgia\n",
      "Original checkins: (332198, 8)\n",
      "Renamed 'local_datetime' to 'datetime'\n",
      "Number of qualified users: 1159\n",
      "Filtered checkins shape: (276308, 8)\n",
      "Filtered check-ins saved to ../data/output/georgia/georgia-filtrado.csv\n",
      "Creating embeddings with weight=0.1, K=7, embedding_size=50\n",
      "\n",
      "Inicando o HMRM...\n",
      "total_places: 23452, total_users: 1159\n",
      "\n",
      "\n",
      "Matrizes criadas...\n",
      "\n",
      "Otimizando os parâmetros\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Shape: (23452, 102)\n",
      "Embeddings for Georgia generated successfully\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-03-08T17:53:46.318247Z",
     "start_time": "2025-03-08T17:12:14.570640Z"
    }
   },
   "cell_type": "code",
   "source": "_ = embeddings_job('texas', path_texas, weight=0.1, K=7, embedding_size=50)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texas check-ins...\n",
      "Created directory: ../data/output/texas\n",
      "Original checkins: (3355419, 8)\n",
      "Renamed 'local_datetime' to 'datetime'\n",
      "Number of qualified users: 11326\n",
      "Filtered checkins shape: (3092354, 8)\n",
      "Filtered check-ins saved to ../data/output/texas/texas-filtrado.csv\n",
      "Creating embeddings with weight=0.1, K=7, embedding_size=50\n",
      "\n",
      "Inicando o HMRM...\n",
      "total_places: 132237, total_users: 11326\n",
      "\n",
      "\n",
      "Matrizes criadas...\n",
      "\n",
      "Otimizando os parâmetros\n",
      "0\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **SVM**"
   ],
   "metadata": {
    "id": "UgeKISxYMs-_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# validação cruzada k-fold no modelo\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "split = kf.split(features_alabama.iloc[:, 0:99], features_alabama.iloc[:, 100])\n",
    "fscores, precisions, recalls = [], [], []\n",
    "\n",
    "acc = []\n",
    "precision = []\n",
    "recall = []\n",
    "w_avg_f = []\n",
    "m_avg_f = []\n",
    "\n",
    "w_avg_p = []\n",
    "m_avg_p = []\n",
    "\n",
    "w_avg_r = []\n",
    "m_avg_r = []\n",
    "\n",
    "for train_index, test_index in split:\n",
    "    X_train, Y_train = features_alabama.loc[train_index].iloc[:,\n",
    "                       :99], features_alabama.loc[train_index].iloc[:, 100]\n",
    "    x_test, y_test = features_alabama.loc[test_index].iloc[:,\n",
    "                     :99], features_alabama.loc[test_index].iloc[:, 100]\n",
    "\n",
    "    model = svm.SVC(\n",
    "        kernel=\"linear\", decision_function_shape='ovo', class_weight=\"balanced\")\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    y_predicted = model.predict(x_test)\n",
    "\n",
    "    precision, recall, fscore, support = score(y_test, y_predicted)\n",
    "    acc.append(accuracy_score(y_test, y_predicted))\n",
    "\n",
    "    fscores.append(fscore)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "    w_avg_f.append(f1_score(y_test, y_predicted, average='weighted'))\n",
    "    m_avg_f.append(f1_score(y_test, y_predicted, average='macro'))\n",
    "\n",
    "    w_avg_p.append(precision_score(\n",
    "        y_test, y_predicted, average='weighted'))\n",
    "    m_avg_p.append(precision_score(y_test, y_predicted, average='macro'))\n",
    "\n",
    "    w_avg_r.append(recall_score(y_test, y_predicted, average='weighted'))\n",
    "    m_avg_r.append(recall_score(y_test, y_predicted, average='macro'))\n",
    "    class_labels = sorted(set(y_test))"
   ],
   "metadata": {
    "id": "xL3EvR5LV7y3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "name_columns = [x for x in class_labels]\n",
    "metrics_f = pd.DataFrame(fscores, columns=name_columns)\n",
    "metrics_p = pd.DataFrame(precisions, columns=name_columns)\n",
    "metrics_r = pd.DataFrame(recalls, columns=name_columns)\n",
    "\n",
    "metrics_f[\"accuracy\"] = acc\n",
    "metrics_f[\"macro avg\"] = m_avg_f\n",
    "metrics_f[\"weighted avg\"] = w_avg_f\n",
    "\n",
    "metrics_p[\"weighted avg\"] = w_avg_p\n",
    "metrics_p[\"macro avg\"] = m_avg_p\n",
    "\n",
    "metrics_r[\"weighted avg\"] = w_avg_r\n",
    "metrics_r[\"macro avg\"] = m_avg_r\n",
    "\n",
    "print(\"\\nMétricas precision:\")\n",
    "display(metrics_p)\n",
    "\n",
    "print(\"\\n\\nMétricas recall:\")\n",
    "display(metrics_r)\n",
    "\n",
    "print(\"\\n\\nMétricas fscore:\")\n",
    "display(metrics_f)"
   ],
   "metadata": {
    "id": "ujjTiMN6Wok9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "melted_metrics_f = metrics_f[[0, 1, 2, 3, 4, 5, 6]].melt()\n",
    "palette = sns.color_palette(\"husl\", n_colors=len(melted_metrics_f[\"variable\"].unique()))\n",
    "\n",
    "sns.boxplot(x=\"variable\", y=\"value\", hue=\"variable\", data=melted_metrics_f, palette=palette)\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Performance Metrics by Fold\")\n",
    "plt.legend(title=\"Fold\", loc=\"upper right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "TUsMjjpnW8An"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analisando as métricas, podemos concluir que o desempenho do modelo na classificação dos POIs com base no embedding gerado pelo HMRM não é muito alto. Isso sugere que o embedding pode não capturar todas as características importantes dos dados de check-in do Alabama, levando a um desempenho relativamente baixo na classificação dos POIs.\n",
    "\n",
    "Porém isso também pode ser por causa da definição dos parâmetros do próprio hmrm, talvez seja bom estudar mais por exemplo o número de componentes latentes (k), peso, tamanho do embedding, etc. **=>** ***se for isso, tenho algumas dúvidas:***\n",
    "\n",
    "***1. faz sentido testar diferentes valores como no exemplo comentado na main? até achar um que dê resultados melhores?***\n",
    "\n",
    "***2. ou esses resultados são satisfatórios já que o MTL \"aprenderia e melhoraria\" as informações?***"
   ],
   "metadata": {
    "id": "zuiwgntJX5LG"
   }
  }
 ]
}
